::: System Designs Notes :::

Goal: we will explore real-life example based system design concepts where we will list down HLD and LLD design for any particular System.

### System Design Fundamentals
    1. Scalability, Availability & Reliability
        1. Scalability
            -> It refers to system's Availability to handle increased load by adding resources - like servers, database instances or network capacity.
            -> Maintains System Performance and reliablity in increased load scenarios.
            -> Types:
                1. Vertical Scaling (Scale-UP)
                    -> simple to implement -> upgrade the current hardware.
                    -> Problem -> Limited by hardware capacity and single point-of-failure.
                    -> eg: upgrade database server from 4 core to 16 core.
                2. Horizontal Scaling (Scale-out)
                    -> Add more machines to distribute the load.
                    -> Better fault tolerance and unlimited growth potential.
                    -> Problem: Need load balancing, distributed data and synchronization.
                    -> eg: Adding more web servers behind a load balancer, Qubes/AWS addes additional pods/instances when demand increases.
        2. Availability Vs Reliability:
            1. Availability
                -> It menas, how long the system can stay up without shutting down.
            2. Reliability
                -> It means how consistently a system can perform a task without failure/Errors.
            NOTE:    
                -> eg: A website can be Availability i.e it loads but unreliable (freq data errors)
                -> eg: A system can be reliable i.e Data loads correctly but is not highly available i.e server mostly stays down.
        3. Design a System that is: Scalable + highly Available.
            ->  HLD Structure:
                1. use "load balancers" to distribute the requests.
                2. Deploy "multiple app Servers" for Scalability.
                3. use replicated Databases with read replicas -> Faster database queries.
                4. Have redundancy across multiple data centers (for Hight Availability)
                5. Use Caching (CDN or Redis) to reduce load.
                6. Implement health checks & auto-restart methods. --> If a instance goes down due to failure, It can automatically restart it.
            -> Flow
                Client ---> Load Balancers ---> Multiple App Servers ---> Distributed Databases (Master + Replicas) + Cache (CND or Redis).
            -> TradeOff:
                1. Balancing consistency + Availability is hardest --> CAP theorm.
    2. CAP Theorm, consistency Models & Database TradeOffs
        1. Why CAP Therom ?
            -> Problem when system becomes Distributed -> i.e more than one machine handling data
                1. If one server crashes what happens ?
                2. What if network Delay happens and 2 uses see 2 different data ?
                3. What if nodes can't talk to each other, should they continue to serve Request ?
            -> CAP therorm - A Distributed system can only guarntee 2 out of the 3 properties at the same time: 
                1. C - Consistency       - Every user sees the most recent write or error     - eg: insta bio updated is immediatly visible to all.
                2. A - Availability      - Every req reserves a resp even if one node is down - eg: insta if one data center is down, still we can post. 
                3. P - Partion tolerance - some servers can't talks but the App still works.
            -> Why best of 2 out of 3 ?
                -> Partion tolerance will happend no matter what  due to network failure or timeouts.
                -> Now In this case 2 choices: 
                    1. Being Consitent - Stop serving untill syncs.
                    2. Being Availabile - Keep serving even if the data can be stale.
            -> Examples: 
                1. CP Systems
                    -> Consitency + Partion Tolerance
                    -> usecase: We cannot allow 2 users to see 2 different data, Thus if one replica is down, we can afford to wait --> temporary unavailablity is ok !
                    -> eg: Banking, Payment Systems or Order system (Here Data correctness is more important)
                2. AP Systems
                    -> Availabile + Partion Tolerance
                    -> usecase: If the data is stale or taking time, that's okay --> but the scroll in social Media must work i.e systems must be available always.
                    -> eg: Social Media Apps, Caches, (Here Availability >> Data consitentany)
                3. CA Systems 
                    -> Consistency + Availabile
                    -> Not any System EXists that implements this 2 together.
            ->  Decision Method:
                -> If there is a network failure (P) --> Do you stop serving stale data (C) Or stay online with eventual Consitency (A) ?
        2. Consitency Model ?
            -> Modern System go with Eventual consitency -> Data might not be instantly same everywhere i.e stale but eventually converges 
            -> eg: DynamoDB, Cassandra or S3.
        3. Database TradeOffs: 
            1. SQL Databases (Relational)
                -> Type: CP Systems
                -> Focus: Data Integrity, ACID Transactions.
                -> eg: PostgresSQL, MYSQL
                -> usecase: Banking, Payments, e-commerce orders.
            2. Non-SQL Databases
                -> Type: usally AP Systems.
                -> Focus: Scalability, Flexible, performace
                -> eg: 
                    - DynamoDB, Cassandra (AP)
                    - MongoDB (can be CP or AP based on config)
                -> usecase: Social Media feeds, logs, caching, high-write workloads.
        4. Modern Perspective:
            -> CAP says what to choose in case of error, When there is no error then we follow: "PACELC"
                i.e if Partion (P) occurs, then choose between Availability (A) or Consistency (C) ELse (E) choose between Latency (L) or Consitency (C).
            -> When network is fine, we trade between Latency and strong consitency.
            - eg:
                1. DynamoDB - AP during Partitions, and prioritizes low latency when no failure(PA/EL).
                2. Spanner  - CP during partitions, but ensures consistency at cost of higher Latency (PC/EC).
    3. Caching & Data Access Patterns
        -> Why we need ?
            -> Performance Improvement -> Process of storing data in a faster storage eg: RAM so that future requests can be served faster.
        -> Caching layers: 
            1. client-side -> Browser, mobile device memory (eg: service workers)
            2. CDN (Content-delivery-network) -> Geographically distributed static assert caching (images or videos)
            3. Appliation cache -> Redis/Memcached near the app servers
            4. Database cache -> Query-level or materialized view cache.
        -> Cache strategices: 
            1. read-through  -> app always read from cache, If miss -> fetch data from db -> stores it.  [Redis + cache-side pattern]
            2. write-through -> app write to cache and db at the same time --> For consistent critical systems.
            3. write-behind (write-back) -> write to cache -> async updates DB later --> high performance system like Analystics.
            4. cache-aside (lazy-loading) -> App hits cache, If missed loads from DB, later update cache --> Common pattern.
        -> Cache Cleaup/Eviction Policies
            1. LRU -> Least recently used -> remove last recently accessed items.
            2. LFU -> least Frequently used -> remove least accessed item.
            3. FIFo -> First in First out -> Remove oldest Item regardless of usage.
            4. Random -> randomly just purge data.
        -> Cache are faster but hold limited memory space -> Must purge data.
        -> TTL -> Time TO live --> automatically removes data after a fixed duration to prevent stale reads. eg: Cache user profile for 10 mins.
        -> Cache Invalidation: 
            -> When data updates -> Cache might hold stale data -> we must sync DB and Cache.
            -> Options: 
                1. Invalidate on write
                    -> Remove cache data when data changes -> Safe but leads to temporary issues.
                2. Update cache on write
                    -> Inconsistent if write on cache fails -> data sync will fail.
                3. Set short TTLs:
                    -> simple but might cause higher DB load.
        -> Caching tech:
            1. Redis -> In-memory, key-val, supports persistence -> most common app-level caching.
            2. memcached -> in-memory, very fast but volatile -> session storage, temporary storage.
            3. CDN -> Static assert, videos, large media
        -> Why Redis preferred ?
            1. Persisitence (snapshortting + AOF)
            2. pub/sub + streams
            3. Rich data types (hashes, lists, sets and sorted sets)
        -> Trade-Offs: 
            1. Fitness Vs Performance   --> Higher TTL improves speed but increases staleness.
            2. Memory Vs hit ratio      --> More Memory --> more storage space -->  fewer misses
            3. Latency Vs Consistency   --> Remote cache (distributed) add latency but centralized data.
    4. Load Balancing, Reverse Proxy & Failover Mechanisms
        -> in prod, we always have more than 1 servers handling traffic.
        -> Prob with just 1 server: 
            1. Get overloaded (CPU/memory bottleneck)
            2. Create single point of failure
            3. Limit Scalability.
        -> Solution -- Distribute the incoming request across multiple servers. -> Job of load balancer. -> No single server getting overloaded, better realiablity and availability and semless horizontal scaling.
        -> Types:
            1. L4 Load Balancer (Transport layer)
                -> works at UDP/TCP level.
                -> Routes bases on Ip and port
                -> Fast but limited routing control.
                -> eg: Amazon network Load balancer, H4Proxy (L4 Mode)
            2. L7 Load Balancer (Application Layer)
                -> works at HTTP/HTTPS Level.
                -> can inspect headers, url, cookies.
                -> can perform intelligent routing -> (eg: /api => Microserve A)
                -> eg: NGINX, AWS Application load balancer.
        -> Load balancing Algorithms:
            1. Round-robin  --> Distributed sequentially (1, 2, 3, 1, 2, 3....)  -> Simple and fair for similar servers.
            2. weighted round-robin -> Assigned weights based on capacity  -> If server 2 > server 1
            3. least connection -> Reoutes to server with fewest active requests => best for variable req times.
            4. IP-hash -> Based on user's IP -> same-user -> same-server  -> useful for session stickiness.
            5. random  -> Randomly pick a server -> faster & statless.
            6. Consitent Hashing -> same key always map to same node --> used in distributed caching or sharding.
        -> Load balancer patterns:
            1. Active-Active
                -> all servers handl traffic.
                -> if one fails, LB redirect traffic to others.
                -> Pros: higher utilization
                -> Cons: more coordiantion needed for shared state. 
            2. Active-passive
                -> one or more servers are on stand-by untill the active one fails.
                -> Procs: simpler failover
                -> cons: Less efficient resource utilization.
        -> What is Reverse Proxy ?
            -> It forwards reqs from client to backend server.
            -> works only at L7 layer i.e HTTPS/HTTP.
            -> eg: NGINX, Apache.
            -> feat: SSL termination, caching, compression.
        -> Health Checks & Failover:
            -> LB periodically pings backend servers --> If they doesn't respond within timeout -> marked as unheadlty -> temporary removed from rotation -> traffic re-directed to backup region.
        -> Gloabl LB:
            -> When users are world-wide, few traffic redirection methods:
                1. based on geolocation -> nearest region.
                2. Latency-based routing.
                3. Weighted rounting (send 70% from US-East & 30% from EU)
            -> Tools: AWS Route53, Cloudflare Anycast, Google Cloud DNS.
        -> trade-offs:
            1. Latency Vs Control  --> L7 is smarter but slower, L4 is faster but dummer [less control on routing]
            2. StateLess vs Sticky sessions  --> sticky sessions simplify state but breaks Scalability.
            3. Active-Active Vs Active-passive  --> More reliable vs resource wastage.
            4. Single Vs Multi-Region  --> Multi-region improves Up-time but add sync complixity.
        NOTES:
            1. What happens when a single LB fails ?
                -> If only 1 LB fails -> system goes down.
                -> Sol: use redundent LBS (active-active or active-passive) or manged LBs (AWS/GCP handles redundancy).    
            2. How do we maintain user serssions across Load balanced servers ?
                -> use sticky sessions (IP hash / cookies) or better -> store session state in Redis or DB.
                -> sticky sessions does prevent scaling, solution is to use distributed session storage to store sessions state.
            3. How do we perfrom zero-down time deployments with LB ?
                -> gradually remove old servers from LB rotation -> deploy -> re-add -> blue-green or rolling deployment.
    5. Database Scaling -> Replication, Sharding and Partitioning
        1. Replication
            -> Copying data from one database server (Master) to one or more servers (slaves or replicas).
            -> improves Performance and availability.
            -> Functionality:
                1. The primary (Master) handles writes.
                2. The replicas (slaves) handle reads. [to avoid conficts]
            -> Types of Replicas:
                1. Synchronous  -  writes is confirmed only after replicas updates     - strong consitency, higher latency.
                2. Asynchronous -  Master confirms immediatly, replicas updates later  - better performance, eventual consistency.
            -> TradeOffs: 
                1. Pros:
                    -> Improves read performance (read scaling)
                    -> Increase Availability (failover possible)
                    -> Enable data locality (geo-distributed reads) 
                2. Cons:
                    -> Write still limited to one master.
                    -> Replication lag (in async setup)
                    -> consistency management is complex.
            -> NOTES:
                - If master crashes, A replica can be promoted to Master manually or via leader election.
        2. Sharding (Horizontal Partitioning)
            -> When data becomes too large for a single server --> split or shard data across multiple servers. -> each shard holds a subset of data.
            -> Shard strategices: 
                1. Range-based     ---  Split by vaule ranges          -->  users 1 - 1000 in shard 1, rest in others.
                2. Hash-based      ---  Hash user_id -> shard number   -->  Even distribution but harder to rebalance.
                3. Geo-based       ---  split by location or region    -->  EU users, US users
                4. Directory-based ---  lookup table maps ID -> shard  -->  flexible but adds lookup oberhead.
            -> TradeOffs
                1. Pros:
                    -> Enables horizontal scaling
                    -> Reduce data size per shard.
                    -> beeter write throughput.
                2. Cons:  
                    -> Cross-shard queries are expensive.
                    -> Harder joins and transactions.
                    -> Rebalancing shards --> Operationally complex.
            -> Rebalancing ?
                -> When shards fills unevenly -> some servers hot, others are cold ---> use consistent hashing to remap keys evenly across nodes.
        3. Partitioning Vs Sharding
            1. Partitioning: 
                -> scale: Within 1 database.
                -> Goal: Manage large tables.
                -> eg: MYSQL partioning a table into data ranges.
            2. Sharding:
                -> scale: across multiple databases.
                -> Goal: Distribute the data across multiple instances. -> horizontally or Geographically.
                -> eg: Splitting users data into multiple MYSQL Cluster.
        -> usecase:
            -> In a large System we combine Replicas and shards to Optimize the performance of the System.
                1. Each Shard may have replicas for read scaling.
                2. Writes -> Master of that shard.
                3. Reads -> Replicas of that shard.
        -> Design Flow: 
            App Server ----> Shard Manager 
                                    |--->  Shard 1 (Master) + Replica 1, Replica 2.
                                    |--->  Shard 2 (Master) + Replica 1, Replica 2.
    6. Message Queues & Async Communication
        1. Message Queue (MQ) ?
            -> It is a temporary holding area where messages (tasks, data or events) are stored until a consumer service can process them.
            -> It allows async communication between 2 services i.e sender and receiver don't have to interact real-time.
            -> How it works ?
                1. Producer -> sends a message eg: "User signed up !"
                2. Queue    -> stores the message reliably (FIFO or priority)
                3. consumer -> Processes message eg: "send welcome email"
            -> Flow Diagram:
                [User Service] ---> [Message Queue] ---> [Email Service]
                    |                   |                    |
                Producer         Temporary store         Consumer
            -> Types: 
                1. Traditional Message Queues  --> RabbitMQ, Amazon SQS, Redis Streams.
                2. Distributed Logs / Streaming  --> Apache Kafka, Pulsar, Kinesis.
            -> why need a Message Queue / Advantages ?
                1. decoupling 
                    Pro:  Services can work independently ! producers and consumer dont have to wait for each other.
                    Cons: Harder to trace flow end to end.
                2. load Buffering
                    Pro: Hanldes burst traffic by queueing requests.
                3. Reliability
                    Pro:  Prevent data loss via acknowledgement and rettries.
                    Cons: Message duplication possible. 
                4. Scalability
                    Pro: Add more cosumers to process high message volume.
                    Cons: Ordering guarantees may break
                5. Async Processing 
                    -> Improves user experience (no blocking operations)
            -> Sync VS Async Communication
                1. Synchronous (Direct Call)     --> API A --> API B (waits for response)  --> Real-time, needs immediate data.
                1. ASynchronous (Queue or Event) --> API A --> Queue --> API B             --> Background Jobs, non-blocking ops
            -> usecase:
                - use Async queues only when the user doesnot need the result immediatly eg: Logging, Analystics, email Notifications.
        NOTES:
            1. At-least-once vs At-most-once Vs Exactly-once delivery ?
                1. At-least-once  --> message may be processed more than once (most common).
                2. At-most-once   --> may loose some messgaes but never duplication.
                3. Exactly-once   --> hard to acheive with kafka transactions.
            2. Dead Letter Queue (DLQ)
                -> A Seperate Queue to store massages that repeatedly fail to process.
            3. Idempotency
                -> Make message processing safe for retries i.e re-sending the same email shouldn;t cause duplication.
            4. Backpressure ?
                -> Happens when the consumer cannot keep up --> Message Queue groes --> implement Scaling ot throttling.
    7. Data Storage Choices - SQL vs NoSQL
        -> Majorly have 2 options
            1. Relational Databases     - SQL    - MySQL, PostgresSQL, Oracle, MSSQL
            2. Non-Relational Databases - No-SQL - MongoDB, Cassandra, DynamoDB, Redis, Neo4j.
        -> Conceptual Differences:
            1. MYSQL
                1. DataModel -> Tables (rows & cols)
                2. Schema    -> Fixed Schema (predefined columns & types)
                3. Query Language -> SQL
                4. Scalability -> vertical Scaling (scale-up i.e bigger machine)
                5. Transactions -> Strong ACID Support.
                6. usecase => Structured, relational data.
            2. NoSQL 
                1. DataModel -> Flexible (key-value, document, column, graph)
                2. Schema    -> Dynamic Schema (fields can vary per record)
                3. Query Language -> Each DB has it's own API or query model.
                4. Scalability -> Horizontal (scale-out i.e more machines)
                5. Transactions -> Often relazed (BASE) for perfromance.
                6 usecase -> unstructured or rapid changing data.
        -> What is ACID principle with SQL databases ?
            1. A - Atomicity
                -> All or nothing. - Either all operations will succeed or nothing will.
                -> us of keyword -> Start Transaction; ....  Commit;   --> Make sure the in-between querie's Atomicity.
                -> usecase: Both debit or credit succeed or both fails.
            2. C - Consistency
                -> Data remains valid.
                -> eg: If a constraing is set of Check(balance > 0) --> transaction with negative balance will fail -> Data must be valid in the tables.
                -> eg: No rules/containts violted.
            3. I - Isolation
                -> Transactions don't interfere interfer with each other.
                -> SQL provides Isolation levels
                    Read uncommitted --> Read Committed --> Repeatable Read --> serializable.
                NOTE: Higher levels = strong isolation = less concurrency.
                -> eg: Parallel transfers donot overlap data.
            4. D - Durability
                -> Changes persist. -> Once the data is commited -> It is permanently stored, even if the system crashes.
                -> Achieved via write-ahead logs (WAL), journaling or replication.
                -> eg: Data survives crash/restart.
        -> When to choose SQL or NO-SQL ?
            1. Reasons to choose SQL
                1. Need Complex queries or joins.
                2. Data has clear relationships (eg: users, order, purchases)
                3. Need strong consistency and atomic transactions.
                4. expect moderate scale but high integrity.
                eg: banking systems, Inventory Management or HRM or CRM applications.
            2. Reasons to choose NoSQL
                1. Data structure evolves quickly or is semi-structured (eg: JSON-like)
                2. Need massive Scalability and high availability.
                3. deal with high-velocity writes or big data.
                4. tolerate eventual consisitency.
                eg: Social feeds and chat messages, IoT -sensor data or Logging and analytics.
        -> Types of No-SQl Databases and when to use which one ?
            1. key-Value store
                -> simple pairs
                -> usecase: Caching or sessions.
                -> eg: Redis, DynamoDB
            2. Document Store
                -> JSON-like documents
                -> usecase: User profiles, catalogs.
                -> eg: MongoDB, Couchbase.
            3. Column Family store
                -> Columns-oriented
                -> usecase: High write throughput, time series.
                -> eg: Cassandra, Hbase
            4. Graph database
                -> Nodes + Edges.
                -> usecase: Social networks, recommendations
                -> eg: Neo4j, ArangoDB.
    NOTES:
        -> SQL databases provides high Consistency (every reads get the latest write) but they scale vertically which limits.
        -> NO-SQL databases scarifice Strong consistency but provides eventual consistency (i.e data will load but not immediatly) and focus on Availability and partition tolerance - ideal for distributed systems.
        -> Modern World Hybrid Appraoch:
            -> Modern System uses a polyglot appraoch - multiple db for different needs.
            -> eg: For E-Commerse System Design.
                    1. User data, Orders, Patments    -> SQL (PostgresSQL)        -> Need high consistency (data must reflect immediatly)
                    2. Feed storage, Product Catalog  -> NoSQl (MongoDB)          -> Eventual consisitency works but must always availabile, For Feed Storage: High write throughput (use CassandraDB)
                    3. Cart/sessions                  -> Redis                    -> memory-storage with key-value pairs, fase-access.
                    4. Seach                          -> Elasticsearch.           -> Full-text search optimized.
                    5. analytics                      -> Column Store (Cassandra) -> High write throughput. 
    8. CDN & Edge Caching
    9. Rate Limiting & Throttling
    10. Design for Reliability - Heartbeats, Leader Election, Quorum, Failover
    11. Monitoring, Logging & Observability


### HLD System Design
    1. Design URL shortner (Bit.ly)
    2. Design Instagram Feed System / Social Media App 
    3. Design Youtube / Video Streaming platform.
    4. Design Chat Application (WhatsApp / Slack)
    5. Design Distributed Cache Like Redis
    6. Design File Storage System like Google Drive
    7. Design Notification / Event Queue System
    8. Design Rate Limiter or API Gateway
    9. Design Ride-Sharing Platform Like Uber
    10. Design E-Commerce Recommendation or Search System

### LLD System Design - Code, Class, Entity.
    1. Design a parking lot system
    2. Design a Library management system
    3. Design a hotel booking system.
    4. Design a Elevator / Lift System.
    5. Design a Meeting scheduler.
    6. Design a Logging FrameWork / Splitwise.

### Extra LLD & HLD Qs:

    1. Airline Booking System.
        1. Function Requirements.
        2. Non-Function Requirements.
        3. Enities/Actors: 
        4. LLD

    2. Event Booking System.
        1. Function Requirements.
        2. Non-Functional Requirements.
        3. Entities/Actors
        4. HLD
        5. LLD


