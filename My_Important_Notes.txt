Notes Or Some Important points:

*****Phase - I Of DSA Concepts and Notes*****

Why #include ?
-- preprocesser directory, This runs even before the code is complied. It imports the files required by our code to execte successfully.

Data Types:
-- 1 byte = 8 bits, Each bit can be 0 or 1.
eg: 4 bytes = 32 bits, Range:  -2^32 to 2^32-1. 
-- Premitive Data Type: int(4 bytes), char(1 bytes), boolean(1 bytes), float(4 bytes), double(8 bytes), long(4 bytes).
-- boolean, 1 -> true, 0 -> false.

TypeCasting:
-- implicit TypeCasting: int a = 'a'; or char ch = 65;  Here char and int can be converted automatically.
NOTE: if large number is type casted to char i.e 4 bytes(int) is casted to 1 byte(char), then only the last 1 byte will be converted to char equivalent.

How negative Number stored in memory ?
-- The first bit(called most significant bit), if 0 -> positive, 1 -> negative.
-- only negative numbers are stored and retrived from memory using 2's compliment.
NOTe: If negative number is stored as unsigned int (only allow positive numbers), then stroing with 2's compliment, but for retrival no 2's compliment. Thus a big number will come.

Note:
-- During operation, the result data type will be the higher data type among the operands.
eg: int/flaot or float/int -> float, datatype of the result.
-- cin, handles input to the program. It ignores whitespace characters('\n', '\t', ' ')
   means, if 2 inputs are seperated by newLine, space or tab, they will be considered as 2 different input with cin.
-- cin.get(), handles the whitespace characters. It accepts the whole input.

Rules of Binary Addition:
-- 0 + 0 -> 0
-- 1 + 0 or 0 + 1 -> 1
-- 1 + 1 -> 0 and carry 1.
-- 1 + 1 + 1 -> 1 and carry 1.

Role of <<(left shift) and >> (right Shift)
left Shift (<<)
  -- All the bits will shift towards left side i.e The number will be multipled by 2 for each shift.
  -- Preferred for small numbers, for Large numbers it becomes uncertain.
eg: 3 << 2 -> 011<<2 -> 1100 -> 12 or 3 * 2^2 -> 12.

Right Shift(>>)
  -- All the bits will shift to right side. i.e the number will be divided by 2 for each shift.
  -- Preferred only for positive numbers. Tailing will be zeros, For negative numbers, tailings will be complier dependent.
eg: 5>>1 -> 101 >>1 -> 010 -> 2 or 5/2 -> 2.

Reason for using namespace std ?
-- Here std is used in front of cin and cout along with scope resolution operator, --> std::cin or std::cout
-- which indicates that the object cin and cout are defined inside the namespace whose name is std. 
-- The std is the standard library, and both cin and cout are defined inside this scope.

-- bool type of function, return 0 means false and 1 means true.
-- One way to find out if num is odd or even, num&1 if true odd else even. (Bitwise And)
-- In Switch statements, you can just switch an int or char that's all.

In an array initialization,
   int arr[5] = {0} -> Initize all values to zero.
   int arr[5] = {1} --> {1, 0, 0, 0, 0} only first value will be non-zero, rest will be zero.
   int arr[5]; std::fill_n(arr, 5, 1); --> it will Initize all values with 1, Any non-zero value you can Initize the array with. [**IMP]
why function handling arr expect size ?
   Because we have situation where arr[10] = {1, 2}, here totalSize is 10 but the size we expect is 2. That's why always send it as param to function.
NOTE: int arr[size], where size is a variable -> Bad practise.
-- Inside For loop, inputs can be passed in a single line space seperated.
-- Arrays are passed by reference, So no copy is made when passed to function. Any update of arr in function will be reflected. 
-- XOR, use, finding unique elements among all duplicates. [**Imp]
   a^a = 0, 0^a = a;
   eg: 2, 3, 1, 3, 2 --> 2^3^1^3^2 -(XOR is commutative)->2^2^3^3^1 --> 0^0^1 --> 1(Unique Element).

Time Complexity Order:
  O(1) < O(logn) < O(n) < O(nlogn) < O(n^2) < O(n^3) < O(2^n) < O(n!)
-- 10^8 Operation Rule --> Today's Mordern Machine can do 10^8 Operations/Second.
   use ? 
   -- Helps to decide which Complexity we can think of for our problem based on the constraints given.
   -- use the chat below: For the given constraint based on n,
      n < 10^8 --> at max O(n) or O(log n)
      n < 10^6 --> at max O(nlogn)
      n < 10^4 --> at max O(n^2)
      n < 2000 --> at max O(n2logn)
      n < 400 --> at max O(n^3)
      n < 100 --> at max O(n^4)
      n < [15..18] --> at max O(2^n * n^2)
      n < [10..11] --> at max O(n!) or O(n^6)
   -- This chart helps to avoid TLE. (Time Limit Exceeded).

Note:
   -- Anything % n --> will always lies between 0 to n (used in cyclic logic of array) [**IMP]

**Kaden's Algo: Max contigious Sum Problem.
   -- 3 Step Process: maxi = first ele, sum =0 and run a loop 0 to n-1
      1) update sum.
      2) put max of maxi and sum in maxi
      3) If sum < 0 , make sum = 0.
maxi will return the max sum of the contigious array.


** Binary Search:
 -- We update mid as (start+end)/2, if we have one situation when start is 2^31 -1 and end also same i.e INT_MAX.
 -- then this case start+end will be out of range of int.
 -- Solution: instead use this for mid, mid = s + (e-s)/2;  (**Very important Edge case**) [**IMP]
 -- NOTE:
    If we are doing finding any pivot element, do remeber to update the h = mid  instead h=mid-1, then condition like while(l<h) and return l as answer, There is a chance it will skip the pivot if you use the later.
 -- we can find sqrt of x using BS as 0(logn) complexity --> logic minimizing the search space from 0 to x. 

-- Very Famous Questions from Binary Search:
Why Binary Search approach ?
-- If we find a mid in a solution space(montonous i.e 1,2,3 ...), and can say like for all element < mid we can ignore and move to  rigth part or we can ignore all ele > mid and move to the left part. 
we can use BS Approach. ===> Basically, we are reducing our Solution Sample Space to a limited options using Binary Search.

** Book Allocation Problem:
   -- you have books with certain pages, find the max no of pages assigned to a student is minimum.
   -- using the BS Sample Solution space.
NOTE: Sample Solution Space: Its a big range of values between one l and one h, somewhere between the solution lies.
   -- Find that solution using BS, and decidie if you want to search left or right of mid.
NOTE: similar Questions like Book allocation Problem:
1) Leetcode's Capacity To Ship Packages Within D Days.
2) Painter's partion Problem.
3) EKO_ SPOJ Problem: Forest and woodcutter, max height of the blade to get atleast k amount of wood.

** Aggressive cow's:
   -- Have n stalls and k Aggressive cows, assign k cows to n stalls such a way that min dist between them is max.
   -- return largest min dist between the cows.

** Cookie and cook:
   -- Have RAting of the chefs given. R rated cook can cooks in R mins 1 dish, next dish in 2R min and so on.
   -- Given the requiremnt of number of Cookies. minimise the time.

*** Very Important NOTE For Binary SEarch problems****
-- All Problems related the binary Search Works on the bases on solution Sample space.
steps: 
-- understand what the problem is asking, Find arr, k and what need to maximise or minimise.
-- Find a range where the solution may lies, Take the mid of the range.
-- See if you can use that mid and find out if there is a chance you can move to right or left to find the solution.
-- Implement the BS algorithm, with a condition check wether to move left or right.
-- Eventutally you will arrive at the solution.

** STLs Basic:
-- Clasiified into 2-parts: Containers and Algorithms.

Containers STL:
* Sequence Containers:
1) Arrays: 
   -- It is the same static array based on a normal array. That's why we dont use it.
2) Vectors:
   -- Dynamic Array. It expands,
   -- When it overflows, It makes it size double. 
   eg: vectors has 1,2, size = capacity = 2, Now add 3, size = 3 , capacity = 4 (since it overflowed the capacity Doubled)
   NOte: when cleared, size becomes zer0, capacity remains the same.
3) deque:
   -- Doubly ended Queue, It is dynamic. 
   -- you can push and pop from both front and back.
   NOTE: 
   -- It is not contigious memeory allocation. It uses some fixed static array and those are kept under track.
   -- Erase(start, end) --> takes iterator range.
   -- No dq.capacity() is there, just dq.size() works, that returns the total number of elements present.
4) List:
   -- It is implemented using doubly linked List.
   -- Direct access of the element eg: v.at(1) or V[1] is not possible here. (**Main**)
   -- Since direct access is not there, erase(post) --> time complexity will be O(n); Note: post has to be an iterator.
NOTE: For Singly Linked list we use: forward_list
   
* Container Adapters:
1) Stacks:
   -- LIFO
   -- TC of all Operations: O(1)
2) Queue:
   -- FIFO
   -- TC for all Operations: O(1)
3) Priority Queue (Very important):
   -- Max heap --> A Queue where the first element is always greatest.
   -- Min heap --> A Queue where the first element is always the minimum.
   -- So You put data as usual in the Queue, But when you start puliing out data,
      If maxHeap --> you will get the greatest element
      If min Heap --> you will get the lowest element.
   -- maxheap is by default, for minheap you need to do: priority_queue<int, vector<int>, greater<int>> Q;
   -- ForEach doesnot work here.
   -- NOTE: It is a queue, but the methods are similar to that of a stack,
      push() --> psuhes in the queue [O(logn)]
      top() --> returns max [O(1)], pop()--> remove the max [O(logn)], empty()--> checks if its empty or not

* Container Associative:
1) set:
   -- Set stores unique elements.
   -- it is internally implemented using BST.
   -- No Modification allowed, Two options allowed, insert and delete only.
   -- It returns elements in sorted ordered.
   -- Count tells if the set contains a particular element or not (**main**)
   -- TC of insert(), find(), erase(), count() --> O(logn)
   NOTE: 
   unordered_set:
   -- It is faster than set.
   -- It stores the elements in random ordered.

2) map:
   -- key-value pairs, all keys are unique.
   -- It is implemented using red-black tree or balanced tree/ Binary Search Tree.
   -- just map, stores all the keys in sorted order..
   -- TC of insert(), find(), erase(), count() --> O(logn) [strictly for ordered Map]
  Note:
  unordered_map:
   -- It is faster.
   -- It stoes the keys in random order.
   -- This is implemented using hashTable.(**main**)
   Benefit: TC of insert, delete, search is O(1) [Way Faster than any other DS]

3) Pair:
   -- Used to Store 2 heterogenous data together. eg: pair<int, char> P1.
   -- How to Store Data ?
      -- P1.first = 5; P1.second = 'A';
      -- Pair P2(5, 'A');
      -- P2 = make_pair(5, 'A') -> This also make a pair using the make_pair method.
      -- P2 = {5, 'A'}
   -- Copy One Pair to another Pair ?
      Pair<int, int> P2; P2(P1)
   -- NOTE: If the pair elements are not initialized, then it defaults to 0.
   -- Swaping pairs ?
      P2.swap(P1); -> swap the elements of P1 and P2. [data types of elements should match]

NOTE:
   -- the time Complexity for Operations like push or insert, pop among containers.
      - For map and set, insert and remove takes O(logn) as they store in sorted order.
      - For unordered_map and set, inset and remove takes O(1) --> random order.
      - For priority_queue --> heapify tech used --> insert and remove takes O(logn)
      - for others like vector, stack, queue -> Operations take O(1).


Algorithms in STL:
Note: header required: #include<algorithm> or #include<bits/stdC++.h> (***It includes all headers**)
1) BinarySearch:
   -- It takes starting post itr, ending position itr and searchElement. O(logn)
   -- It returns 0 if ele not present or 1 if ele is present.
   -- lower_bound returns itr of the ele and itr of ele - v.begin() gives the postion of element.  //O(logn)
   -- lower_bound:
      -- If ele exist -> returns the iterator.
      -- If multiple ele exist --> return the first index iterator.
      -- If doesnot exist --> return the next max index interator.
   -- upper_bound:
      -- If ele exist --> return the iterator of the next big ele.
      -- If multiple ele exist --> return the iterator of the next big ele after the last occurance
      -- If ele exist --> return the iterator of the next big ele.

2) min, max and swap and reverse.
3) rotate: It rotate the element at the given pivot point.
4) Sort: 
   -- It sorts the array between the given positions.
   -- It is based on Intro sort that include: insertion, quick and heap sort. 
   -- Worst Case: TC: O(nlogn) SC: O(n2)
NOTE: 
   -- It takes a 3rd boolean param, can be used to define a custom sort.
eg: 1) sort(arr.begin(), arr.end(), greater<int>()) -> sort the arr in descending order.
    2) 
    bool compare(pair<int, int> p1, pair<int, int> p2) {
      return p1.first > p2.first;
    }
    sort(arr.begin(), arr.end(), compare);  -> sort based on pair's first element.



** Sorting Techniques and Notes ***
Selection sort: 
   Logic is just start from left and look for the min elem towards right, swap it with the start index and move on...
   TC: worst Case(If array is reversed): O(n2), BestCAse(if array is alrady sorted.): O(n2)
   UseCase ? --> User whenEver the size of array is small.

NOTE: Stable and unstable sorting algorithm:
   -- An algorithm is called stable if two objects with the same key appers in the same order after sorting.
   -- It mostly matters if we have duplicates in array, If all element are distinct it doesnot matter.
   -- BAsically, Order of the element before sorting = Order of the element after sorting i.e stable sorting algorithms, Order of the elements are preserved.
   -- eg: 
      Stable Sort: Bubble sort, Insertion Sort, merge sort, count sort.
      Unstable Sort: Heap Sort, selection Sort, Quick Sort.
   -- Also Converting any comparison-based unstable sort to stable sort is possible, we need to take care of the position of the ele, while comparing.

Bubble Sort:
   -- Logic: comapare and swap neighbour elements and this takes the largest elemnt to the last.
   -- useCAse: In any ith Round, placing the ith largest element in its correct place.
   -- TC: O(n2)
   -- scope of optimization: If any round, There are no swaps then just break away, means The array is already sorted.
   -- TC for best case i.e if the array is already sorted, Then TC: O(n)

NOTe: In-place and Out-Place Sorting:
   -- In-place Sorting means the input and the output occupies the same memory and no extra space is needed while sorting.
   -- Out-of-place sorting need extra space to work for sorting. soting that needs O(n) like merge sort are put under this.
   -- below O(n) extra space, Sorting like heap and comb sort falls under in-place sorting.

Insertion Sort:
   -- Logic: Basically, Take the ele one-by-one and place them in between the elements where it belongs.
   -- NOTE: No Swaping here, Insertion Sort is a stable sort.
   -- For left to right, the array will get sorted slowly as we place elements in right place by shifting the array to right.
   Why Insertion Sort ?
   -- It is an adaptive Algorithm. It knows till where you need to go for placing the element at its place. So less comparisions.
   -- It is a stable sorting algorithm, The Order of the ele before and after sorting remain same in case if duplicates present in the array.
   -- If n is small Or If the array is partially sorted then its best algorithm.
   -- TC:
      Bestcase: If the arr is already sorted, then All we need is only 1 comparisions, So O(n)
      Worstcase: If the arr is reversed, The Complexity is O(n2)    

***Handling Char Array and String in C++***

-- Char array type String is the C-way for Handling string in c++.
-- string in c++, can be used to handle string in c++. It comes as a String Class support in c++.
-- Strings in C++ --> 1-D Char Array.
-- cin>>str works in c++. It ends the String with '\0' i.e a null character used as terminator.(use to detect where the string is ending !)
-- Char array useful Methods:
   here s1 and s2 are char arrays. 
   strlen(charArr), strcmp(s1, s2)[Note: returns 0 if equal], strcpy(s1, s2);[NOTE: s1 and s2 must be same size]
NOTE:
   -- Cin stops taking input till it detects: space, tab or enter.
   -- to accept String with spaces, use variable of dataType String and getline(cin, varName); **IMP**
NOTE: getline takes a thrid param a char that you want to set custom delimeter. eg: getline(cin, str, '*'); //It will read upto '*' and ignores the rest.
   -- AlphaNumeric ASCII Range: (always Remember)
      0 - 9  -> 48 to 57
      A  - Z  -> 65 to 90
      a - z -> 97 to 122
   -- iswalnum() : Built-in STL method, Takes char input. and decide if its AlphaNumeric or not. (***V.IMP***)
   -- stoi(str): Converts a number from string to integer. eg: stoi("123") --> 123. 
NOTE: It filters out anything that is not number and returns just the number. eg: stoi("123 pass") --> 123.
      If any char is before numbers then it will throw error.(**V IMP**)

-- String initialization: string str("sourav") or string str = "sourav";
-- String useful methods:
   1) getline(cin, str), str.length(), tolower(char), toUpper(char), str.push_back()[NOte: here param should be a char] and pop_back(), str.copy(charArray, len, startIndx) -- returns a character array List.
   2) str.substr(strt, len), str.append("str")[NOTE: Here param should be a string, add the string at the end], str.compare(str2) --> return 0 is equal.
   3) str.replace(start, len, s2, substart, subLen) --> can be used to replace a particular char or string at a position.
   4) str.erase(post, len), str.insert(post, s2, subpos, sublen);
   5) str.find(s2, post) --> returns the first index where it is found, post --> length of the search str.[NOTE: It returns a post if found else a -1] eg: int found = str.find(s2);
   6) to_String(int val) --> takes any numerical Value and returns String. **IMP**
NOTE: push_back works faster than appened.

-- String stores data in the from of a null terminated array but in normal usage it wont allow you to use it.
-- Diff between Char Array and String:
   -- String is a class, char array is just an array of char.
   -- String allocate memory dynamically.
   -- String Provides lots of inbuilt methods.
   -- Downside: String are slower, compared to char array.
NOTE:
   -- Str doesnot respect any '\0' if it comes in the middle of the string while printing. It will ignore it if cout<<str;
   -- NOTE: Here '\0' will be considered while calculating length.
   -- Char array if you have '\0', it will stop printing as soon as it hit '\0'.

String Question Trick:
-- If you need a freq table, you can use flat alpha freq table. its O(1) since its constant. i.e char lookupChar[26] = {0}; //Always 26 character.
-- To add the freq of a char ch,eg: lookupChar[char-'a']++; [** condition: If the given string in lowercase: char-'a', If in upperCase: char-'A', If not sure, --> Convert to lowercase and then check]

NOTE: 
   -- '' --> char, " " --> string in c++.
   -- Methods used for char i.e charArr --> strlen(chArr), strcmp(chArr1, chArr2), strcpy(chArr1, chArr2),
      --> iswalnum(ch), stoi(str), tolower(ch), toUpper(ch), str.push_back(ch) --> addes a char at the end..
   -- String methods:
      -- str.length(), str.substr(start, len), str.find(str, len), str.erase(pos, len), str.append(" ") --> adds a string..



****Multi-Dimensions Arrays****
-- 2-D Array represented as a linear array in memory using the formula: (columnLength)*rowIndex + columnIndex
-- int arr[][] is not accepted in c++. We have to specify colIndex for mandatory. **IMP** (This identifying the column is difficult, But can be handled.)
-- How to Declare 2-D Array with vector:
   vector<vector<int>> matrix(n, vector<int>(n, 0)); //creates a n x n matrix with all element initialize to 0.

***Maths For DSA***

1) Seive of Eratosthenes
Problem: Find the no. of prime no strictly less that n.
-- assume all are prime no.
-- mark all ele that are divisible by current ele as non-prime.
-- at the end left out numbers are prime.

TC: O(nlog(logn)) -> everytime we remove all the ele divisible by a prime number. thus, it becomes n*HarmonicProgression of prime numbers.

2) Euclid's Algorithm:

GCD: Greatest Common Divisor or Highest common Factor
-- maximum common factor that divides both the number. that is GCD of those 2 numbers.
-- As per Euclid's formula: gcd(a, b) = gcd(a-b, b) or gcd(a%b, b) [a%b or a-b will give the same result]
-- HOw to use this formula:
   keep on putting the formula until either the first or second parm is becoming zero.

NOTE: Relation between LCM and GCD
      LCM(a, b) * GCD(a, b) = a* b

3) Modulo Arithmetic:
-- we know for sure, if a%b then ans always lies between 0 to n-1.
NOTE:
In Few questions, its given the ans range should be below 10^9+7 i.e. 100000007.
-- This means you need to use res%1000000007 everytime you perform any operationto keep the res within 10^9+7 range.

Formulas:
1) (a+b)%m = a%m + b%m
2) a%m - b%m = (a-b)%m
3) a%m * b%m = (a*b)%m

** Fast Exponentiation:
Why ? -> for finding a^b, Noraml logic of multiplying a btimes take TC: O(b).
      -> using Fast Exponentiation method it will be O(log b).
use --> use to fast find the x^n value.
Formula:
for a^b --> if b is even then (a^(b/2))^2
        --> if b is odd then (a^(b/2))^2 * a       
code Logic: for a^n
-- if n is even keep on multiplying a, and update as n as n/2.
-- if n is odd then return res = res * a.
NOTE: even n starts with even, at the end one time n will be 1 i.e odd, thus it will return the result correct.
eg: 2^5   a^n
res = 1
n= 5 -> odd, res = 1*2 = 2, a = 2*2 = 4, n>>2
n= 2 --> even, 4*4 = 16, n>>2
n= 1, res = 32 (ans)

NOTE: 
Always use bit operations whereever possible, it is faster then binary operands.
eg: For checking odd/even --> a&1 true--> odd, false--> even.
    For finding a%2 --> a>>1 , right shifting bit by 1 is same as divide by 2;


********Phase - 2 for DSA and NOtes****************

** Pointers in C++ **
-- symbol Tabler: 
   Its a DS created and maintained by the Compiler, It stores all the information about the variables and its scope.
eg: int num = 5; symbol Table will store the mapping of 'num' with the address of 5 in the memory.
-- '&' --> address of Operator, Fetch the address of a variable. Note: address will be in hexaDecimal format.
-- '*' --> derefernce operator, It will go fetch the value at the given address.
-- why pointer ?
   Its can stores the address of any variable i.e it can holds the hexaDecimal Address value.
Imp NOTE on Pointers: 
-- Always Initize your pointer with some value. eg: int *p; --> this will point some garbage memory location.(Never do like this)
-- Size of Pointer is always 8 irrespective of the dataType. AS it stores just address always.
-- For Arithmetic only addition and subtraction allowed on pointers.
-- If we say ptr = ptr + 1 where ptr is a pointer to int type. If ptr was storing 100 address, ptr = ptr+1 will br 104 as int is 4 bytes.
-- The differenct between the address will be irrespective of the type of the variable.
eg: arr[] = {2, 3}, address of 2: 100, address of 3 = 104, but address of 3 - address of 2 is 1. (Here the 4-bytes based on int dataType is not considered)

Types of Pointers:
1) Null pointer: a pointer that points to nothing or Null.
eg: int *ptr; --> this just points to a garbage memory location.
int *ptr = NULL OR int *ptr = 0 --> points to a null location. 
2) Double pointer: A pointer that stores the Address of another pointer.
eg: int num = 5; 
int *ptr = &num; int **qtr = &ptr; --> here qtr stores the address of pointer ptr. For accessing the value, **qtr is same as *ptr is same as num.
3) Void pointer: A generic Pointer, It is a pointer that has type void.
Main Advantage: It can be used to store any address of a variable of any type i.e it can store address of int or double or char.
Drawback: cannot be dereferenced and no operation can be performed on void pointers.
eg: int num=5; void *ptr = &num; --> ptr can stores the address of num.
4) Wild pointer: A pointer declaration without initialization. Basically it points to some garbage value.
eg: int *ptr; --> a wild pointer. now if next you do, ptr= &num; now it's no more wild :)
5) Dangling pointer: Pointer pointed to a location which is free or deleted.
   Situations:
   1) Out of SCope situation: If ptr is pointing to something inside the scope and you try to access ptr outside the scope.
   2) free the pointer: Trying to access a pointer that is already freed.
   3) in function call: If a fun returning a address of a local static variable. then in main function it wont be able to fetch the address of the same. Reason: call stack of fun would be destroyed.

Pointers & Integer Arrays:
-- Pointers can be used efficiently for handling arrays. Ref: see the Phase2 PointersBasic.cpp for reference.
NOTE: (Important)
-- We cannot update the array name like arr = arr +1; It is now allowed and throw error.
   If we have array arr points to address 100 (As stored in symbol table), Then it cannot be changed atall.
   arr = arr + 1; --> this will throw error, as you are trying to update the location of array.
--Instead of using array name, We can use pointer to move in the array. eg: int *ptr = arr; ptr += 1; --> this is valid.

Pointers & Char Arrays:
-- In char array, If you give an address of a position from the str, it will print all the element till the end of the array till \0.
-- name of the array for integer array will print address of the first ele but in char array it will print the entire array.
-- Never declare String like char *ch = "sourav". It will not stop printing till its gets a null pointer in th memory.
Important NOte:
-- If the char array len is fixed and a pointer is pointing to the first char address, Then its okay. It will print the entire char till the end where its hit \0 char.
-- But if you have a char where you just assign some character. with no len defined and a pointer is pointing towards it, then it will keeps on printing till its hit the \0 char. (Never Recoomended)

Fucntions & pointers:
-- If we pass a function to a pointer then we can play with the vlaue but not the address.
-- Array is passed as a pointer to the function.
Benefit:
-- We can send a part of arry in the function if we want. eg: getSum(arr+4, n-4); where n is the size of original Array.

*** Static and Dynamic Memory Allocation **
Refernece Variable:
-- same memory but we had different name for pointing to the same address.
How to decalre it ?
eg: int i=5; int &j = i; //thus i and j both points to same memory location.
Why need ?
-- we use it in pass by reference to a function.
-- basically the same memory location is shared by fn and the main method.
NOTE:
Pass BY Value: 
-- here a copy of the param is sent to the function. and it exists till the fun life cycle.
-- Original and the copy takes seperate memory location.
(**IMP**)
Bad practise:
-- Never Uses Reference variable as return type of Function. eg: int& fun() {...}
Reason: Basically here you will be returning a refernce of a local variable from inside the fun. But after fun is done, It memory is cleared from the fun stack.
-- Same reason if we never use pointer as return type of fucntion.
-- Declaring arrays like: int arr[n]; Never do like this, declaring size of array at runtime is bad practise.
Arrat size should always be declared first. then getting assigned at run-time.
Reason Why ?
-- When a prog start, it brings 2 types of memory, Stack(samll space, depends upon space required by prog.) and heap(big space). 
-- sizeOf stack memory << sizeOf heap memory.
-- If array size is assigned runtime and not enough space in stack memory, then prog will crash.

NOTE:
-- Allocating memory in stack Memory -> static
-- Allocating memory in heap Memory -> Dynamic

How to Allocate memory dynamically ?
-- using new keyword, --> allocates memory under heap and returns the address of it.
-- pointer is used to catch this address and access this space. 
eg: int *ptr = new int; //Craeting a int type in heap.
-- Creating array in heap: 
   eg: int *arr = new int[n]; //here since this memory is allocated in heap, this is acceptable and can be used.

NOTE: Key difference between static and Dynamic allocation
-- Staic allocation take fixed space in stack memory, Dynamic allocation takes: data Type space in heap + 8 bytes for pointer in stack.
-- Static allocation, the memory is auto-released after execution finished in stack memory, But in heap, the memory needed to release manually. 

Very Important to Release The memory in heap:
-- deleting memory from heap, delete ptr;
-- deleting array memory from heap, delete[] ptr;

** Dynamic Matrix **
-- Dynamic matrix can be created like: 
size: m rows and n columns
int **arr = new int *[m]; //this will create m Rows that takes address.
then use a for loop: arr[i] = new int[n]; //this will create n columns for each row.
-- Releasing memory in matrix, we first release the columns row-wise and then release all the rows.

NOTE: (Very Important Always Remember)
Whenever, we use any memory in heap, we need to always manually release the memory.
-- In Stack memory it get automatically release when the execution overs.



*** C++ Keywords ***



Macros:
-- #include: preprocesser directive, It brings pre-programmed functionality in our code workspace. eg: for cout and cin functionality we need iostream.
-- #define: we can use it to define macros.
what macro ?
A piece of prog in the code that is replaced by value macro. eg: #define PI 3.14 --> Now we can use PI for 3.14 everywhere throughout the program.
Advantage:
-- It prevents updation of the value by mistake.
-- It doesnot occupy any additional memory, thus faster.
How macro Works ?
-- Before Even the code is compliled, everywhere in the code the macro item is replaced by its value.
Types Of Marcro:
1) Object like macros: It replace some keyword with some value. eg: #define PI 3.14
2) Chain Macros: Two or macro related to each other
eg: #define AGE 25
    #define WEIGHT (Age+25)
3) fucntion Macros: we can have functions replacing in the code using macros.
eg: #define AREA(l, b) (l*b)
4) Multi-line Macros: we can have multi-line macro. NOTE: need to use backslash-endcharacter
eg: #define ELE 1,/
                2,/
                3
      we can use this maco like: int arr[] = {ELE}; // arr = {1,2,3}

Global Variable:
Why we need ?
-- We want to share variable between the functions.
Ways to do ?
1) we can use Reference variable to share the variables between the functions. (Recommended)
2) use global Variable and use scope operator to access to whereever needed in the prog.
NOTE: Bad Practise to use global variables in the program.

Inline functions:
What ? -- Used to reduce the fun call overhead.
NOTE: Only Functions having 1 line in body can be converted to inline. by adding the inline keyword before the function call.
-- whereEver the function is called, the function call is replaced with the function body.

Default Params:
-- If the value is passed, it will consider it, if value is not passed it takes the default value.
NOTE: (**IMP**) 
-- Always start assigning default values in the params from right to left manner.



*** Recurssion *** (***VIMP****)

-- Basically when a function calls it-self.
3 step in Recurssion:
1) Base condition: 
   -- The exit condition, 
   -- Return is mandatory in this part.
2) Processing Part:
   -- ANy processing involved.
3) Function call part:
   -- The same function is called itself with some updated paramter.
-- The Above order is called Tail-Recurssion. If Function call first and then Processing, it's called head recurssion.

Approach for DEaling with Recusrsive problem:
1) Try to solve only 1 case.
2) come up with a Recursive Realtion Formule. eg: for fibo its F(n) = F(n-1)+F(n-2);
3) come up with a base case. NOTE: It must return someting. eg: in fibo, if n==1 return 1 and if n==0 return 0
4) then write the recursive relation.
Note: (**IMP**) 
-- Always try to make the Recursion Tree.
-- During dry run, always consider the recursive function as f(n), and track the changes and the function stack.
eg: how we can vizualize the recurssion Tree during dry run ? Like this in case of sum of the arr using Recursion.
first param -> current start position of the array, 
second param -> len of the array.
arr = 4 5 6
fn(0, 3) -> 4 + fn(1, 2) -> 4 + 11 = 15 (answer)
fn(1, 2) -> 5 + fb(2, 1) -> 5+ 6 = 11, thus return 11
fn(2, 1) -> Hit the base case, return 6 since ar[2] is 6 

Real Logic behind Recursion while solving questions ?
-- Try to find the base case first.
-- Then try to solve only 1 case related to the problem, Rest Recursion will handle automatically.


Sorting Algorithms:

1) Merge Sort:
logic ?
-- First step: break the entire array using finding the mid formula, Divide it till you reach at single-single element block. (use Recursion)
-- Now we will merge all the single-single element block into together in a sorted way.
-- At the end: You will have an Sorted array.

Why we use Merge Sort ?
-- It is faster than any other Sorting Algorithm so far. TC: O(nlogn)
-- It is used to sort Larger dataset faster.

Approach:
-- You have one big array. Using mid, divide that into 2 array.
-- using recusrsion sort the 2 halves of the array.
-- then merge them into one sorted array. (ref: Merge 2 sorted array).

One Freq asked Implementation of merge Sort: (****VIMP***)
Inversion Count Problem 
- how far the array is from being sorted ?
Condition for Inversion: For any i and j, 
1)  i < j 
2) Arr[i] > Arr[j].
Number of such i,j pairs gives the Inversion count.
Logic:
-- same logic of sorting using merge sort.
-- to count inversion, While merging left and right array, if ith ele of left is > jth element of right.
since the aray is sorted, all the right elements of left Array are bound to be grater than jth element of right array.
Thus, this gives a count while merging 2 sorted array. 
-- we need to keep track of this count while Recursion.

2) Quick Sort 
Logic ?
1) First choose the first element as pivot element. Then find the correct place for the pivot element i.e pivot index.
   -- Iterate the Array, Counts ele smaller than the pivot element. then currentPost + count gives the pivot element index.
   -- Put the pivot ele at pivot index and then make sure for element left of pivot <= pviot element.
   -- All element on right of pivot must be > pivot element.
   -- return pivot index.
2) Use Recursion to solve the left side of pivot element, then right side of the pivot element.
  
Time Comlexity Analysis: 
-- Toataly based on Which one you choosing as pivot element.
-- We consider the TC of Sorting around the pivot element as O(n).
Worst Case: When the pivot ele is choosen as the start or end index, Then TC is O(n^2). --> Recursive call and Sorting.
Average Case: When the pivot element is chosen as mid element. TC: O(nlogn) --> The array breaks in half around pivot.
Best Case: TC: O(nlogn) --> Any other method of choosing pivot element. 

NOTE: 
-- Quick Sort is not a stable sort (Order of the keys with same val might change during swaping)
-- Quick Sort is in-place. (only Recursive call takes space in memory as in for variables no extra space.)

NOTE: (very Important)
Two Ways of implementing and choosing the pivot element and make sure for all ele on left <= pivot and all ele on right > pivot ?
1) Hoare's Partiion Algo: (Preferred)
  -- Two Pointers i and j, one at the start and another at the end, Both comes toward pivotIndex.
  -- and then swap it accordingly to satisfy the condition. all ele before pivot <= pivot and all ele after pivot > pivot.
2) Lomuto partition Algo:
  -- When pivot element is choose as the last element.
  -- We use a two pointer, i and j both are at the first element.
  -- As we move right using j pointer till 2nd last element, if arr[j]<pivot, we swap with ith index and increment ith.
  -- At the end, swap 2nd last element with the i+1th element, Thus you have your pivot index as i+1th index.

NOTE: (**very very Important**)
Why Merge Sort is Preferred for LinkedList and Quick Sort is Preferred for Arrays ?
Reason:
-- Random access is possible in Arrays i.e I can get arr[5] in O(1) time. But in LinkedList random access take O(n), if you try to access nth element.
-- On the other hand, In LinkedList, Insertion take O(1) time.
Thus,
In Merge Sort, By logic we break the array from mid and then create a Sorted array, Thus For Linked List this is best.
In Quick Sort, By Logic we are swapping a lot. Since Random access takes less time for array, For Array this is preferred.

NOTE: [Important]
-- While Handling the Variables in Recursion logic, Never pass var++ to recursive function. instead use var + 1. 
-- If we use unitary increment or decrement, it going to stuck in a infinite loop.

[IMP]
** Common Pattern in solving Recursion questions like:
   powerSets, subSequences and keypad problems [Important]...
You break the Problem using recursion, you use 4 main params in the recursion function.
   input -> passed to the function from main.
   output -> stores the result till a recurcive function reaches base-case.
   index -> Each stage of recursion, we increment it by 1 on the input.
   ans -> when it reach base-case, we store the data from ouput into ans. NOTE: this param we pass it by reference as we need to return this from main function.

** Basic Backtracking logic of Letter of Combination(Keypad problem):
Baically, at a particular stage in the Recursion,
   -- we added a character to output String.
   -- Then we call the recurcive fucntion with the updated output.
   -- After this call is done, We removed the character that was added.
Thus, the output is back-tracked to the previous value before adding the character. [This is called backtracking]
   -- In the Next Recursive Iteration, we have the original Value of ouput
   -- A new charater is added and Recursive fun is called.
   -- After this is done, we are poping again the character that is added. [Backtracking]

NOTE: Pattern while solving Questions with Backtracking..
  -- It is the normal logic where you do Recursion. and moves down the recursion Tree.
  -- Catch is, When you return, and move up in the Recursion Tree, you want to return to the same state as it was before moving Down the tree. [ This is backTracking]
  -- so, after the Recursion call, you do some logic, to return to the same state as it was before.

Time Complexity in case of Recursive Function:
2 ways of Finding TC for Recursive Functions:
  1) By using the Recursive Relation Function --> Analysis the functions as a function of the len of array given. Frame Time Equation and solve.
      -- First of all try to frame a Recursive realtion for the algorithm. eg: F(n) = n * f(n-1) + K;  K is for base case and Recursive call goes for next n-1. 
      -- use that to get a Time Relations i.e T(n) = K + T(n-1), K time for some operations and baseCase, T(n-1) time for solving the next n-1.
      -- then furthur break it down and solve it to get the Time Complexity.
      NOTE: How to solve this T(n) functions ?
      -- Just write all the realtions from T(n) to T(1), then add all of them, to get the final time complexity of the algorithm.
 2) By judging seeing the Recursive Tree of the function. 
      -- Analaysis any pattern from the Recursive tree. Get the Time Complexity.

TC of some Recursive alogortithm:
  1) Factorial: T(n) = n*T(n-1),  TC = O(n)
  2) Binary Search: T(n) = K + T(n/2),  TC = O(logn) --> T(n/2 because Recursive calls goes to the half of n).
  3) Merge Sort: T(n) = K + T(n/2) + T(n/2) + T(n),  TC = O(nlogn)
NOTE: here, K is time for base case and finding mid, T(n/2) -> recursive call to Left, T(n/2) -> recursive call to right, T(n) -> for merging 2 sorted array in one.
  4) Fibonacci Series:  T(n) = T(n-1) + T(n-2), solving this is difficult.
Note: here, Let assume each node f(n) has 2 nodes, one for f(n-1) and f(n-2). [By judging from REcursion Tree].
   then for next node we will have 2 nodes of f(n-1) and f(n-2) has 2 nodes, total 4 nodes. and so on...
   Thus, From the pattern, TC -> (2^n) [Worst in Time complexity].

Space Complexity of Recursive Function:
2 steps to Judge:
1) Find the Recursion Dept of the function.
   -- Recursion dept, How many times the function is called recursively. same number of functions will be there in the callStack.
   -- Each Call, consumes how many Space ? 
   NOTE: HEre even if some constant Space is used, If the dept is n function calls, then Total Space consumed is O(n).
2) Check if any additional Space is used in Each of the call Stack ?
   -- If additional space is used, Since the function is called Recursively, n times of that space will come in the final Space complexity.

How to get Recursive DEpt ?
  -- Get it by analysising the Recursion Tree of the function.

NOTE: [IMP]
-- TC of an algorthm is well defined i.e we have O(n), O(logn), O(n2) and so on. Basically can be represnted in a graph and functions.
-- Space Complexity of algorithm in case of REcursion is very uncertain and cannot be defined using any funciton.
   Thus, In the Complete execution time of the function, the maximum space it has taken in the callStack is considered as Space complexity.

Space complexity of some Recursive alogortithm:
  1) Factorial: F(n) = n*F(n-1),  
     The Function is called n times, Thus, Recursion Dept is n and each Fucntion assumed to take some constant, k space,
   SC: O(n).
  2) Binary Search: F(n) = K + F(n/2)
     The Recursive Dept will be called log n times, Each time it takes constant, K space, 
   SC: O(logn) 
  3) Merge Sort: F(n) = K + F(n/2) + F(n/2) + F(n),  
     -- This function will be called log n times, Since every time the function is dividing the array from mid.
     -- Now To solve each part, WE are calling merge function to merge 2 sorted array. Here we are taking n space(at the maximum), we visit all the ele atleast once.
   SC : log n + O(n) or O(n) [space complixity of merge sort] (since logn <<<<< n, we can ignore logn)
  4) Fibonacci Series:  F(n) = F(n-1) + F(n-2)
     -- Here, The Recursive dept will be n(at maximum, on the left side of the recursion tree), 
     -- In each some costant space k is taken. 
   SC: O(n)

NOTE: Solving Questions using the Recursion Tech [VIMP]
   Everytime we solve a question using Recursion, Always remember:
   -- There has to be a base case or the REcursion logic should be inside a if block, To stop the recursive function call.
   -- YOu need to solve just one case, The rest you assume, Recusion already have handled it. [For logic formation]
   -- Do a dry Run, for the approach, using the Recursion tree or Fun call stack.
   -- Space Complexity of a Recursive Function, depends upon the how many times the Function is calling itself i.e Recursive dept.

*** OOPs Concept ***  Ref: https://www.codingninjas.com/codestudio/guided-paths/basics-of-c/content/118817/offering/1381799

Love babber OOPs RoadMap: https://whimsical.com/object-oriented-programming-cheatsheet-by-love-babbar-YbSgLatbWQ4R5paV7EgqFw

Objects 
  -- Entity which have 2 things, Properties and some behaviour.
  -- Instance of a Class
Class -> Its a User defined data-type.
-- Size of class is the total space the property of the class occupyies.
NOTE:
  -- Size of Class with no Property is 1 byte.

How to access Class from a seperate file in the program ?
  -- You need to add the File in the header. eg: #include "ClassFileName.cpp"
  -- After including this, you can directly use the class in the Program.

Access Modifiers ?
NOTE: By default, Class properties and methods are private
1) Public: Everywhere in the WorkSpace.
2) Private: Only inside the class, used my method functions
3) Protected: Accessable by the child class methods

Getters & Setters ?
  -- Methods inside the function, They are public and can access the private property of the class.

Padding and Greedy Alignment of memory ?
 -- Size of Class or struct with different types of data-types might differ because of padding and greedy alignment of memory by the compiler.
 eg: If Struct contains data Types of int, int*, char and int* -> Total size expected -> 4+8+1+8 = 21, But Size will be 32.
   Reason ?
   -- Comipler add padding for int and char type and try to match with int* size, 
      Thus size of int -> 4+4(padding added), size of char -> 1 + 7(padding addde). and int* will be 8 bytes. so total -> 32.
 NOTE:
   -- THis padding depends upon the order of the Property defined. Here it will greedyily align them the memory. [IMP]
   eg: If the order of the data types changes, int*, int, char, int*  -> Total size will be -> 24.
   Reason ?
   -- [int*] -> 8 bytes 
   -- [int,char and padding] -> 4+1+3 (Compiler greedyily adds padding and accomodate int and char and  make it equal to 8]), 
   -- [int*] -> 8 bytes, 
   Total -> 24 bytes.
   
   Conclusion: [V.IMP]
     -- space allocated to class or Struct, It depends upon what type of data-types it holds and in what order and based on the compiler how it will arrange them.
     -- One thing Observed,
     If we defined data-type with more space first, It trys to greedyly arrange multiple data-type of lower space after it within the range of higher data-type space.
     eg: if data-type order is --> int, long, char -> (4+4[padding]) + 8[long] + (1+7[padding]) -> 24
         but if we rearrange  --> long, int, char -> 8[long] + (4[int]+1[char]+3[padding]) = 8+8 -> 16. [Class/struct takes less space]

Creation of Object for class:
   Static way: Car c1;
   Dynamic way: Car *c2 = new Car(); OR Car *c2 = new Car;

Constructors:
  -- Invoked Whenever the object is Created.
this keyword in Class:
  -- 'this' is a pointer to the current Object of the class.[NOTE: this is pointer type, eg: this->ObjProperty]
  -- It basically stores the address of the current class's Object.

NOTE: [V.Imp] 
  -- Whenever you defined any constructor in a class, The default constructor is removed my the compiler for that Object.
      eg: You created a class, you created an Object. eg: Demo d1; -> [Default Constructor by the complier is called]
      now, you defined a parametrised Constructor. eg: Demo d2(23, 34) [Parameterised Constructor created by you is called] [NOTE: the default constructor by compiler is destoryed]
      now, you try to call the default constructor. eg: Demo d3. [NO Constructor Found --> Error by the complier]
  -- If any of the constructor is defined, Then defualt constructors will be removed.
     -- If you define just parametrized constructor, and you want to use default constructor ? It will not be availble.
     Solution: You need to explictly define a deafult constructor, now Demo d3, will work.

Copy Constructor:
  -- It is automatically generated when the Class is created.
  -- It Copies One Object to another Object for the same class.
  -- The Object passed as Param to Copy constructor will be always passed by Reference [VV.IMP]
  Reason:
     -- If passed by Value, It bassically copies the data, So copy constructor is needed, Thus it's stuck in a loop. [NOt used]
     -- If passed by Reference, It is basically points to the same location with a different name. -> same Object is passed to Copy constructor param with a different name.[Perfect]

  shallow and deep Copy ?
  -- This Method is genrally deals with copy of value in case of Pointer class properties.
  g: A class, has obj1 Object that has one Property char *ch[strores address of a char array]. We will try to create obj2 by copy data from obj1.
  1) Shallow Copy:
     -- Default Constructor follows Shallow Copy. 
     -- when we copy obj1 to obj2, both will store the same address location, what ch is holding.
     Thus, Any change in ch will refelect in both obj1 and obj2.
  
  2) Deep Copy:
     -- Overwrite the default copy Constructor.
     -- BAsically we will create a new pointer and copy the data of ch from obj1 into this.
     -- assign this to the current Object ch eg: this->ch, and Thus, you now have ch pointing a different memory location for Obj2.

   NOTE:
   -- Both Default and Copy constructor are created by the compiler by default when Object is created.
   -- Unless you try to overwrite it, For the new Object created afterward, the default constructors by complier will be overwritten. 

   Copy Assignment Operator:
   -- You have 2 Objects, obj1 and obj2, 
   -- obj1 = obj2, It will copy All the properties from obj2 to obj1.

Destructor:
  -- Used for memory deallocate for the allocated space for the class.
  -- invoked when the Object is getting out of scope.
  -- Takes no Parameter, no Return Type.
  NOTE[VV.IMP]:
    -- Objects which are created static way, For them destructor will be called automatically.
    -- Objects Dynamically created, Destructed needed to be called manually.
    eg: delete obj1; --> this will trigger the destructor.

Static Keyword in Class
  -- Any method or property defined as static, Belongs directly to the class.
  --Class methods dont have access to this.
  -- Objects can access it, but not Recommended. It's basically a class property.
  eg: static int num1; static void tesFun() {...}; [NOTE: Static member functions can only access static property]
  How to use this static Propertires ?
  int ClassName::num1;  -->  we use the scope resolution Operator (::)

NOTE On Class and Constructors: [V.IMP]
   -- Never initialize any Class peroperties. -> You Just declare the properties.
   -- Use Constructor to initialize the properties during Object Creation.
If Not follwed, It will show segmentation Error.




*** OOPs 4 Pillars/Concepts ***


1) Encapculation [Information hiding/ Data hiding]
   
   Defination:  wrapping up of data members(class properties) and functions(class methods).
   -- Fully encapsulated class:  All Data members should be private scoped.
   Why ?
   -- It helps us achieve Data Hiding feature -> More security.
   -- We can make read-only Class. 
      HOW? 
      --> All members are private and assume we only have getter methods.
   -- Unit Testing is easy.

2) Inheritance
   
   -- child Class(sub class) inherits the properties of all the parent class(super class).
   -- Mode of Inheritance: child class can specify the mode in which it wants to access the parent class. eg: Class child: public/private/protected Parent {...};
   -- Private properties from  parent/Super class cannot be inherted
   NOTE: 
   1) If mode is Public: All inherited methods OR properties will be consided as same mode as it is defined for parent class.
   2) If mode is protected: All inherited methods OR properties will be considered as protected.
   3) If mode is Private: All inherited methods will be private to the child class.
   
   ** Protected Access specifers:
   -- It cannot be access outside the class, Only can be accessed inside by the methods.
   Private Vs Protected ?
   -- Protected properties can be accessed by the child class vs Privtate properties are accessable to no one.
   
   Types Of Inheritance:
   1) Single Inheritance:  class A  -> class B
   2) Multiple Inheritance:  class A, class B  -> class C
   3) Hirarchical Inheritance: Class A -> Class B, Class C 
   4) MultiLevel Inheritance: Class A -> Class B -> Class C 
   5) Hybrid Inheritance: Mix and Match of the above mentioned Inheritance logic.      

   Inheritance Ambiguity ?
   -- If class A has fun1(), class B has fun1(),
   -- Class C inherite Class A and Class B, fun1() of which class it will call ?
   Solution: We use (::) Operator to resolve the ambiguity.
             eg: C obj1; obj1::A.fun1() -> This will call fun1 of class A.
                         obj1::B.fun1() -> This will call fun1 of class B.

3) Polymorphism: 
2 Types: 
   1) Compile-Time
      1) Function Overloading:
         -- In a class, function having same name but different number or type of params.
         NOTE: Functions with exactly same name and same param but different return type is not considered to be Overloaded. [Imp] 
      2) Operator Overloading:
         -- We can Overload an Operator to do custom Task, syntax: returnType operator+(input) [input should be passed by reference in case of binary Operators]
         NOTE: There are few Operators which cannot be overloaded --> :: , .*  , .  , ?: 
         -- eg: Addition of two complex numbers, using the '+' operator.
   
   2) Run-Time:
      -- Method Overloading. [Mostly happend in Inheritance Case]
      -- Class A has method dowork(),
         Class B inherits Class A, and define it's own method doWork().
         In this case, The Inherited method doWork is overwritten by class B. [Very useful in practical Situations]

4) Abstraction: 
   -- Basically Implementation hiding from the User.
   eg: Class have access modifiers like private, Thats hides and protect data of the class.

Observations from Inheritance:
   -- Protected class properties or Functions cannot be accessed by obj outside the class.
   -- Memeber Functions of a class should always be under the public Access specifers.
   -- Inheritance Ambiguity --> Related to Multiple inheritance -> solution: use (::) class name then method to call the specific method.
   -- Run time Polymorphism, Method Overloading --> Can be found during inheritance, When parent and child class have same method name.
      By default, Child object can access it's own function. If not defined, then it will call the inherited function from parent class.




*** Linked List ***  [Ref: https://www.codingninjas.com/codestudio/guided-paths/data-structures-algorithms?source=youtube&campaign=YouTube_CodestudioLovebabbar23rdJan&utm_source=youtube&utm_medium=affiliate&utm_campaign=YouTube_CodestudioLovebabbar23rdJan]


   -- Linear Collections of Nodes. Nodes -> capsule that conains data and address of next node.
   -- It allocates Memeory in Random fasion, No continious memory allocation.
Why ?
   -- Arrays are rigid DS. During Run-time, The size cannot be increased.
  NOTE: 
      In vectors, When Size exceeds, a new vector is created with more size and all datas are copied from old vector to this. [**not optimal case**]   
Importance ?
   -- Dynamic DS, Size is adjustable during Run-Time, no memory wastage.
   -- Insertion/Deletion in the middle is Faster and invloves no shifting of elements.   

Types: 
   1) Singly Linked List: Node contains data and one Address pointer, next.
   2) Doubly Linked List: Node contains data and 2 Address pointers, next and prev
   3) Circular Linked List: The Tail points back to head. [NOte: Here we use just tail pointer OR head pointer any one is fine]
   4) Circular Doubly Linked List: Tail points to head.

Working with Node:
   -- Consider creating node in the heap Memory, using new keyword. [heap memory allocation is prefered for node]
   -- Also passing nodes to methods, Pass it by reference. [VIMP]
   Reason ? -> we will update head, So if we dont use by reference, Head update will not reflect outside the function.


NOTE: Important Concepts on Linked Lists Questions.

** Detecting If a LinkedList is Circular or not ?

1) Iterative Method: 
   -- Start ahead of head Node and keep on traverseing.
   -- till you find the end or come back to head Node Again. If reached head again -> circular else Not.

2) Using Map as a lookup for Visted Node.
   -- Use Map to Store the Node Address as key and a bool flag to denote if the node is Visted or not.
   -- If Visited already, The Linked List is either having a loop or Its Circular in Nature.

*************** Important Concept Of loop Detection and Handling in LinkedList ***************

NOTE: In Case we our Linked List has Loops, Then The above Iterative Method will break, the temp pointer will stuck in an infinity loop.

Possible Concepts That we can Expect from a Looped LinkedList:

   1) Detect if there is a cycle present in the given LinkedList or not ? [using Map Loopkup or Floyd's Loop detection Algo]
   2) Find the Node where the Loop is starting ? [using Floyd's Loop Detection Algo]
   3) Remove the Loop From the Looped Linked List. [Find the start of the Loop, Then REmove the loop]

** Loop Detection **
Approach:
   1) Using Map as a lookup for Visted Node.
      -- a Lookup Map that stores key as Node Address and value as a bool flag that if the node is visited or not.
      -- Same Logic as mentioned Above.

   2) Floyd's Loop Detection Tech: [VIMP]
      -- Take 2 pointers, Fast pointer[moves 2 nodes at a time] and slow pointer[moves 1 node at a time].
      -- Travese till the fast becomes NULL, 
         IF anytime Fast and slow pointer meets, The Linked List having a loop.
      -- If not then, The Linked List dont have a loop.

** Loop Starting Node for Looped Linked List **
Approach:
   -- use Floyd's Algo to find the Point of Intersection of fast and slow Pointer.
   -- move slow pointer back to head.
   -- increment slow and intersection point 1 node at a time untill they intersect. [This gives the begining of the loop]
NOTE: For Proof Why this approach works ?  Look in LB Notes -> phase2 -> LinkedList4.pdf

** Removal of Loop for Looped Linked List **
Approach:
   -- Use Floyd's Method to find the start of the loop,
   -- Since its a loop, use iterative way to traverse loop nodes and remove the loop.

Important Concepts on LinkedLists:

* Finding mid Node in Linked List, using Fast and slow Pointer method: TC: O(n) SC: O(1)
Appraoch:
   -- Handle edge case is Head is null or only 1 Node is there.
   -- First declare 2 Node Pointers, slow on head and fast on the next element of head.
   -- Now Run a loop, till either fast becomes NULL or fast->next becomes NULL i.e Fast reaches the tail Node.
   -- Move slow 1 Node at a time, and fast 2 Node at a time.
   -- AS fast reaches the end Node, Slow gives the mid Node.




*** Stacks ***


-- Data Structure that follows: LIFO [Last In First Out]

Implementation: All operations: TC: O(1)

1) Using STL:  stack<dataType> s; [Operations: s.push(), s.pop(), s.top(), s.empty(), s.size()]
2) Using Class: 
   -- Stack can be represented in class using 2 DS: Arrays OR LinkedList.
   -- Mandatory Class Properties For Stack using Array: A dynamic Array[constructor will take care], Array_size, top_index.

Important Questions From Stack: [Level: Hard][Very Important]
-- Largest Rectangle in Histogram
-- Celebrity problem [Very Important]
-- Find the Largest Rectangle in a Binary matrix. [Logic: Use Largest Area in Histogram]
-- N Stacks in an Array [Logic: Use topIndex[] and nextFreeSpace[] and freeSpotIndex For managing stacks]
-- Design a MinStack Class with TC: O(1) SC: O(1) [Logic: use pair to store element in Stack, OR Computaion of previous and current minVal]

NOTE: 
   -- To Fill all the elements of the array with a particular value, use:  memset(arr, 1, sizeOf(arr)) [This will fill all elements of the arr with 1]




*** Queues ***


-- Logic used in Queue: FIFO [First In First Out]
-- Iterator used in Queue: front[keep tracks of front queue element] and back [keep tracks of queue back element]

STL Queue Operations: queue<int> q;
Operations: 
 -- q.push(x) , q.pop(), q.size(), q.front(), q.back(), q.empty() [TC: O(1)]

NOTE: pop here in q returns nothing. Use q.front() to get the data before poping.

[VIMP]
For Implementaion of Normal Queue and cyclic Queue and DEque [Remember the Conditions For Push and Pop Operations]
  -- Majorly 3 conditions you should check before moving front and raer...
     1) check if the Queue is Full or empty based on the front and rear.
     2) Rest front and rear if only 1 element we are inserting or poping out.
     3) update the front or rear to maintain the cyclic condtion.
     4) Noraml increment/decrement of the front and rear.

Input Restricted Queue ?
  -- Push Operation is allowed only at rear. [push_back Operation]
  -- Pop Operation is allowed at both front and rear. [pop_front and pop_back Operation]

Output Restricted Queue ?
  -- Push Operation is allowed at both front and rear. [push_back and push_front Operation]
  -- Pop Operation is allowed at front. [pop Operation]

DoublyEnded Queue ?
  -- Push Operation is allowed at both front and rear. [push_back and push_front Operation]
  -- Pop Operation is allowed at both front and rear. [pop_front and pop_back Operation]

** Uses of DoublyEnded Queue ? -> Can be used to implement stack as well as a Queue.
** Practical Use of Queue -> CPU Scheduling, Process Scheduling Operation [OS]

STL DoublyEnded Queue Operations: deque<int> q;
Operations: 
 -- q.push_back(x), q.push_front(x) , q.pop_back(), q.pop_front(), q.size(), q.front(), q.back(), q.empty() [TC: O(1)]

Important Questions of Queue..
   -- First Negative Number in Window of K
   -- First non-repeating Character from a stream.
   -- Circular tour [VVImp]
   -- Interleave first half of queue with second half [stack approach and queue appraoch]
   -- Implement N Queues in Array. [same as Implementing N stack in Array] [Vimp]
   -- Sum of minimum and maximum elements of all subarrays of size K [Vimp, Hard]




*** Binary Tree ***


 -- non-Linear Data Structure [One Node may connect to multiple Nodes]
 -- BinaryTree: Each Node may have <= 2 childs [at max 2 childs]
 
 Imporant Terms:
 -- Node: Each Element is called Node, that holds the data and pointer to the next Node.
 -- Root: Top Node is Root Node.
 -- Parent and Children Nodes.
 -- siblings: Node at same level For a parent.
 -- Ancestor and Descendent: For Node, Look 2 level up or down.
 -- Leaf Nodes: Nodes with No children.

Creation of basic BTree: 
   1) Using REcursion we can create.
   2) If we have the levelOrder Data, we can crate a Btree.

Travarsal in Binary Tree:
   -- LevelOrder traversal [Queue is used]
   -- Reverse level order traversal [same method as above, use stack to reverse the content of Queue.]
   -- InOrder traversal [Left Node Right, Recursive]
   -- PreOrder traversal [Node Left Right, Recursive]
   -- PostOrder traversal.[Left Right Node, Recursive]
   -- Morris Traversal [Based on creation and deletion of temp Links] SC: O(1) [VIMP]

NOTE: 
   -- The whole Advantage of Morris Traversal is, It can iterate a BTree in TC: O(N) and SC: O(1). While all other algo use Recursion or Queues. TC: O(N) and SC: O(N)

Important Questions from Trees:

BASIC: [Pattern: Use Recursion + pair, TC: O(n)] [Instead of calling height for each NODe, Use Pair to calculate height at each node]
   
   1) Height of binary tree / find the max dept of a given Tree. [Use Recursion]
   2) Diameter of a BTree. [VIMP, use pair<Diameter, height>]
   3) Check for balance Tree. [VIMP, use pair<isBalanced, height>]
   4) Check if 2 trees are identical [use Recursion]
   5) Sum Tree [VIMP, use pair<isSumTree, Nodedata>]

Traversal Questions: [Pattern: In BTree, LevelOrder Travesal and horizontal Distance of Nodes and Map]

   1) Zig-Zag/Spiral Traveral. [Queue + Processing level Order with tempArr + alterantingFlag]
   2) Boundary Traversal. [leftTraverse + leafNodes + rightTravese]
   3) Vertical Traversal. [VVIMP, horizontal Distance(hd), + MAP + LevelORderTraversal] [Hard]
   4) Top View Of BTree. [Vertical Traversal Logic + map stores HD and firstNodeVal ]
   5) Bottom View Of BTree. [Same as top View, map stores HD and LatestNodeVal]
   6) LEft View of BTree. [Level Order Traversal, map stores nodeLvel and firstNodeVal>] / [Rcursion + LevelTracking]
   7) Right View of BTree. [Level Order Traversal, map stores nodeLvel and LatestNodeVal>] / [Rcursion + LevelTracking]
   8) Diagonal View of BTree. [Level Order Travesal, map stores diag and List of all Nodes] / [Recursion + diagonal Tracking, using map]

NOTE: [**VIMP**]
   -- Recursive way is always faster than Level order Traversal. [Map Operations adds to TC, makes it O(nlong)]
   -- In REcursion, If want to track something as you REcursivly call, REmember to pass the data by reference.
      Else, When Recursion backtrack, The value will change as it was from where it is called. [As callstack moves down, Values gets updated, Only by reference passed variables maintains there value]
   -- For look_up freq table or map used for lookup, Always go for unordered_map.
      1) unordered_map stores data in a unordered way so, insertion and fetching data is faster. 
         -- implemneted using: Self-balancing BST Tree. eg: Red-black Tree. 
         -- TC of search, insertion and deletion: Average Case: O(1), WorstCase: O(n)
      2) map stores the data in a sorted way, so inserting and fetching data is slower. 
         -- implement using: hashTable 
         -- C of search, insertion and deletion: WorstCase: O(logn)

Important Questions:

   1) Longest Path from Root to Leaf Sum [Recursion/ Level order traversal]
   2) Lowest common Ancestor (LCA) of Btree [Recursion] [**VIMP**]
   3) Sum of K Nodes [IMP, Recursion + vector to store Node path values]
   4) Kth Ancestor Of a Ndoes. [Recursion] [**IMP**]
   5) Find the max Non-ADjacent Node Sum. [Recursion + pairs<Sum including curr Node, Sum excluding the curr Node>]
   6) Construct Binary Tree From Inorder and preOrder. [Recursion + iterate preOrder from left to right]
   7) Construct Binary Tree From Inorder and postOrder. [Recursion + iterate postOrder from right to left]
   8) Min Time to Burn Binary Tree. [ParentNodeMapping + VistiedNodeMap and Queue] [***VIMP**]
   9) Flattern a Binary Tree into LinkedList. [Morris Traversal, SC: O(1)] [**VImp**]




** Binary Search Tree ** [Mostly asked Topic]


   -- A BTree, with Strict 2 Conditions: 
   For any Node,   
      1) The left Child should be smaller i.e currNode->left->data < currNode->data
      2) The Right Child Should be greater i.e currNode->right->data > currNode->data
   
   -- TC of insertion of a new Node in BST: O(logn) [As based on the value we either traverse left or right]

NOTE: 
   -- Inorder Traversal [LNR] of BST is always sorted. [**VVIMP**, Used at multiple logics]
   -- Min in the whole BST is at the left-most Node. Max in the whole BST in at the right-most Node.
   -- Inorder Predecessor and InOrder Succesor [**VIMP**]
      In a BST, at a particular Node:
      1) Predecessor: The max element from the left tree of the currNode.
      1) Sucessor: The min element from the right tree of the currNode.

Basic BST:
   1) Creation of BST [TC: O(logn), Use Recursion]
   2) Seraching in BST [Use Recursion, TC: avergaeCase: O(logn), worstCase: O(n)[In case of skew BST]]
      NOTE: Recursion taking SC: O(N), Use Iterative way to do in SC: O(1)
   3) Deletion in BST. [Recursion, SearchingNode + DeleteNode Conditions ] [**VVIMP**]

Important:
   1) Validate a BST [Recursion, Check for BST cond for each Node, should fall under the range]
   2) Kth Smallest/largest Element in BST.[inorder Recursion, Morris Traversal [Faster Algo]]
   3) Inorder Predecessor and Succesor of a Node in BST. [maxNode from LeftTree[predecessor], minNode from rightTree[Succesor]]
   4) LCA[Lowest common Ancestor] of a BST. [Compare the Root val with nodes]

   5) Two Sum in BST [Inroder BST traversal + 2-pointer approach]
   6) Falttern a BST to sorted LinkedList [Inorder BST Traversal + Node Link Manipulation]
   7) BST to Balanced BST [Inorder BST + Create new BST from mid Node using Recursion]
   8) PreOrder to BST [use the Range Logic + Recursion to Find the place to put the Nodes] [**IMP**]

NOTE: While using REcursion with vector Traversal: [**VVIMP**]
   -- If you using say index to iterate each element of the vector in a Recusrion Appraoch,
      1) index should be defined as pass by refernece.
      2) If you are passing index+1 in the recursion call, It will throw error. [reference error]
   Solution:
      -- Update the index first and then pass the updated value in the recursion call.
      -- Make sure any var you passing as reference, You cannot update it directly in REcusion call, You need to update value first and then use that updated value..

** VVIMP Questions:
   9) Merge 2 sorted BST [InorderTraversal + Vector/LinkedList + contructBST]
      ** Approach1:    
         -- InorderNodes Array from BST
         -- Merge 2 sorted Array 
         -- InorderNodes Array to Balanced BST
      ** Approach 2: [Space efficent than approach1]
         -- Convert BST to Inorder doubleLinkedList. [**IMP**]
         -- Merge 2 Double LinkedList
         -- Inorder Linked List to Balanced BST. [**IMP**]
   10) Largest BST in a Binary Tree [using Recursion + class with 4 info for each Node]

NOTE: 
   -- 10th question shows how to handle/hold multiple data using class [or we can use struct also] during recursion.
   -- If only 2 data you need to handle/hold, then go with pairs, If more than 2, use struct or class to hold as data-members.




** Heaps **



What are Heaps ?
   -- Heaps are Complete BinaryTree with heap Order Property.

1) What is complete Binary Tree (CBT): 
   A Binary Tree that satisfy 2 conditions: 
      -- All Nodes must have 2 Children except the last Level.
      -- At any level of the Binary tree, The Node should be filled from left to right order.
2) Heap Order Property:
      -- Max Heap: At any Node, The Node val strictly greater than it's children. 
      -- Min Heap: At any Node, The Node val Strictly smaller than it's children.

How to implement Heap ?
   -- Heap can be implemented using a normal Array, Filling the Node value level wise from 1st index onwards. [**IMP**]
   -- If Parent Node index is i.
      -- Left child index: 2i [1-based index i.e 0th postion of array we wont consider]
      -- Right child index: 2i+1 [1-based index]

NOTE: [** VIMP **]
   -- For 0-based index i.e If the 0th position we are considering...
      -- left Child index: 2i+1
      -- right CHild index: 2i+2
   -- For 0-based Index,
      -- n/2 to n-1 are all leaf Nodes and n/2-1 to >=0, we need to heapify for building heap.
   -- for 1-based Index,
      -- n/2+1 to n are all leaf Nodes and n/2 to > 0, we need to heapify for building heap.

Operations Supported with Heap:
   1) Insertion in Heap [TC: O(log n)]
   2) Deletion in Heap [TC: O(log n)]
   
   3) Build Heap and heapify [ **VIMP** ]
      -- Heapify TC: [O(log n)] -> Taking a Node to it's correct Position in the heap down the Complete binary Tree.
      -- Building a Heap, TC: O(n) -> From arr to heap, using heapify.
   
   4) heap Sort [TC: O(nlogn)]  [**IMP** ]
      -- Given a Heap Nodes, Sort it.

NOTE: 
   -- Important Property of Complete Binary Tree:
      -- If we represnet CBT in array[1-based] of size n, then all element from n/2 + 1 to n are leaf Nodes.   
      -- If we represnet CBT in array[0-based] of size n, then all element from n/2 to n-1 are leaf Nodes. 

NOTE: 
   -- Suppose you using a maxHeap that stored elements as: pair<int, int> eg: priority_queue<pair<int, int>> max_heap;
   -- So based on which int, It would arrange the elements in heap ?
      -- Whatever is the first int in pair, based on that the whole heap will be updated. [Since we dont pass a seperate compare in maxHeap]

**** using the Comparator function in minHeap **** [**IMP**]
   -- So maxHeap implementation is pretty straigt: prioirty_queue<int> max_heap;
   -- For minHeap, it takes 3rd param as a comparator: priotity_queue<int, vector<int>, greater<int> > min_heap;
      Senario: If we are using priotity_queue for any other dataType other than int we cannot use greater<int>.
      eg: If we want to store pair in the priority_queue and sort based on 2nd Value:
   eg:
      //Create a comapartor Class for the same...
      Class Compare {
      public:
         bool operator()(const pair<int, int> &p1, const pair<int, int> &p2) {
            return p1.second - p2.second;
         }
      }
      
      priority_queue<pair<int, int>, vector<pair<int, int>>, comapare> min_heap;

NOTE: Basically, we create a Compare Class and overload the Operator method, with 2 params that we are expecting...
      Now it can return 3 things: -1 [Then the comapre put this value in front], 0 [it will not touch the value] or 1 [It will put the value at the end]


Implementing Heap using STLs:
   -- We need to include: #include<queue>
   -- Max Heap: priority_queue<int> pq_max 
   -- Min Heap: priority_queue<int, vector<int>, greater<int>> pq_min
   -- Operations: pq.push(x), pq.top(), pq.pop(), pq.size(), pq.empty() etc..
NOTE: 
   -- pq.top() and pq.pop() in case of max Heap points to the maximum element and in case of min Heap points to the minimum element.

** Basic Questions:
   1) Find the kth smallest/largest Element in Array [max/min Heap]
   2) Is The Given Binary Tree is a heap [Check if CBT and check heapify Condition]
   3) Merge 2 Heaps [Combine Both heaps + heapify]
   4) Min Cost to connect Rope [use min Heap]
   5) Convert BST to Heap [Inorder + Pre-Order Node Replacement [For minHeap] OR Post-Order for MaxHeap] [**IMP**]

** Imporant Questions:
   1) Find Kth Largest Sum SubArray [Find all Pairs + MinHeap for kth largest]
   2) Merge K sorted Arrays. [use minHeap + Insert First Elements Appraoch] [** VVIMP **]
   3) Merge k sorted Linked List [use minHeap] [Same Appraoch as Arrays]

** Hard Questions: [VeryImp]
   1) Find the Smallest Range in Given K sorted Arrays [use MinHeap + Insert First element Approach]
   2) Find the Median in a Stream [MaxHeap-Median-MinHeap Algo] [**VVVIMP**]
   3) ReArrange a Given String such that no 2 adjacent characters are same. [charFreq + maxHeap] [Imp]




** Hash Maps **


Why needed ?
   1) Major Advantage of using HashMaps -> The Complexity of insertion, deletion and others are better in hashMap
   2) When we need to store data in a form of key-value pair, we use HashMap.

Implementation:
   1) ordered_map: [The data is stored in sorted way based on key]
      -- Implemented using BST.
      -- TC for insert/delete/search -> O(logn)

   2) unordered_map: [The data is stored in random fasion]
      -- Implemented using HashTables.
      -- TC for insert/delete/serach -> O(1)

-- Basic Operations with HashMap:
   1) Insertion:
      -- Map Stores the element in form of key,value pair -> Thus, We can insert a pair directly.
         eg: map.insert(make_pair("key1", value1));
      -- You can insert directly also:
         eg: map["key1"] = value; //Adds a new pair to the DS.
   2) Search:
      1) map.at(key)  -> Returns the value corresponding to the key...
         Problem: If the key dont exist, It will throw error, thus, this is least preffered way to Search.
      2) map[key1] -> Returns the value for the key1.
         Solution: If the key don't exist, It will return 0. [Best way to search/extract Key out of Map]
      NOTE: map[key1] if key1 dont exist, It adds a entry in the map with default value 0. [**VIMP**]
   3) count:
      -- map.count(key) -> Return either 0 or 1.
      -- Checks if the given key is present in the map ot not.
   4) erase:
      -- map.erase(key)
      -- Removes the key from the Map.
      -- If the key is Found and removed, It returns 1.
      -- If the key is not Found and unable to remove, It returns 0.
   5) Traversal:
      1) using auto keyword 
         -- each key,value pair is extracted as a pair Data Structure.
         -- Each element can be accessed using element.first and element.second
      2) using iterator 
         -- Declare the iterator for the map DS and initialize it from map.begin()
         -- Iterate till you reach the map.end() and For each, You get a pair Data Structure.
      NOTE: Iterator is pointer i.e Points to the begin and end of the MAP, You access the value, need to use `->` like pointer.

[**IMP**]
** Implementation of Map using Bucket Array [using Hashing Techniques] [Implementation behind unordered_map]
   -- One biggest Advantage of unorder_map is Insertion/Searching/deletion: O(1)
   -- Bucket Array 
      -- A normal Array, Where the Value is stored in the Array and Key is mapped to Index.
      -- Key is mapped to index based on hash function.

** Hashing Function -> HashCode + Compresion Function.
   1) HashCode -> Job is take the key and  Return a unique integer.
   2) Compresion Function 
      -- It makes sure the integer returned by the HashCode stays inbound with Array Lenght.
      -- eg: If HashCode returned x, then Compresion Function converts to x%sizeOFArray. [For Better Result SizeofArray is taken prime]

**  HashCode Functions and Collosion and Collosion handling Functions.
   HashCode:
      -- It can be a identify Function. i.e It returns the input as output without any modificiation.
      -- Also, Possible You gave input a String and it returned an interger.
         -- It can get the output by adding the ASCII code of all char or any other algorithm.
      -- Thus this is how HashCode converts the given Input.

   NOTE: In general, HashCodes are functions which ensures uniform distribution. It also ensure consistence.
      -- The Same input will only return a particular Output eveytime and Also reduce the chances of any possible collision.

   Collision:
      -- It is highly posible, For 2 different input, HashCode is returning the same output.
      -- This is called Collision.
   
   Collision Handling Techniques:
      1) Open Hashing
         -- Very Popular and also known as seperate chaining [**IMP**]
         -- Basically, If For 2 different inputs we are getting the same result, We Maintain a list at that position in bucket array.
         -- Everytime, we get a result at the same location, we just add it to the list.
         -- It works very fast by this method. 

      2) Closed Addressing
         -- Baically, here, If 2 diff inputs are giving the same index.
         -- Simple push the first one at the result position. The next one for the same result put at the next free location.
         -- The next free space is given by a function. Hi(a) = H(a) + fi(a)
            here fi(a) is a probing function, and i is attempt for accessing the index.
            Types: 
            1) Linear, fi(a) -> i
                  -- For first attemp say you pushed at 7th index in bucket array.
                  -- For secong attemp at 7th index, As per fucntion Hi(a) = H(a) + i  = 7 + 7 = 14.
                  -- So, Inserting at 14th Position.
            2) Quadratic, fi(a) -> i^2
                  -- For first attemp say you pushed at 7th index in bucket array.
                  -- For secong attemp at 7th index, As per fucntion Hi(a) = H(a) + i  = 7 + 7^2 = 56.
                  -- So, Inserting at 56th Position.
            and so on...

** how It is faster in Case of Open Hasing ? [**IMP**]
   -- N -> Total No of Entries in the Map.
      b -> Numbers of buckets avaible in the Bucket Array.
   -- Then For better hashing, we maintain:  N/b < 0.7  [N/b is called Load Factor]
   -- If N increases, then b also should increases in order mainatin the condition.
      -- This is called reHashing, It increases the bucket Size.




** Tries **


   -- It is type of Data structure. useCase eg: It can be used to Implement a Dictionary [Keep track of words insertion/searching/deletion].
   -- It can keep tracks of Words inserted, can search a Word and also delete a particular word.
   
Drawback of using HashMap:
   -- HashMap can be used to keep track of words, But it can give TC: O(1) only in avergae case.
   -- In Worst Case, HashMap TC: O(l), l-> length of each word

Implementation of Tries:
   -- Tries looks like a tree. [Not binary Tree as it may have multiple Nodes connected to it.]
   -- So we have a root Node, IT can store next address of all other ChildNodes.
   -- Each Node Structure:
      1) It needs a variable to hold the data.
      2) It needs an array to hold the pointers to all children.
      3) It needs a bool flag, to indicate the last character while inserting.

Operations with Tries: [useCase: Tracking Words]
   1) Insertion: [Implemented using Recursion] TC:O(length of word)
      -- Each Character is taken From the word one by one.
      -- For each Character, It checks If the Root Node holds the address of that character ?
         -- If yes, It moves the pointer to that Node.
         -- If not, It just create a new Node, attach it to the currNode and move to that newNode.
      -- And the Process Repeats untill all char from the String is inserted.
   NOTE: While inserting the last char of the String, The isTerminalNode flag is made true, Thats tells the whole word ended at that Node.

   2) Searching: [Implemented using Recursion] TC: O(length of word)
       -- Each Character is taken From the word one by one.
      -- For each Character, It checks If the Root Node holds the address of that character ?
         -- If yes, It moves the pointer to that Node.
         -- If not, Then the word is not present and it return false.
      -- And the Process Repeats untill all the char from the String is Searched.

   3) deletion: [Implemented using Recursion] TC: O(length of word)
      1) Approach1: Search the Word char by char in trie and For the last Node, make the isTerminalNode, false. i.e In search for It will never find this.
         Problem: The Char of the deleted word still exisits.
      2) Approach2: While searching char by char in the trie, [**Best Appraoch**] [**Once check the code**]
         -- make the isTerminalNode of the last char of the word false
         -- Also remove words which stores No Pointer to the next char. [i.e You removing characters which are part of this word but not other words]

   4) StartsWith: [Implemented using Recursion] TC: O(length of Word)
      -- Traverse the chars, Find if all of them exists, return True, else return false.

NOTE:
   In Implementing Trie for Words, Possible Operations: Insert/search/delete/startsWith,
      -- All have the same TC: O(length of the word, as we insert char by char)
      -- The Approach for all Operation is similar, Travesing one char to another of the word using Recursion.

Why For Storing Word, Trie is better than using a HashMap ?
   Hashmap: map<string, bool> i.e <word itself, flag to indicate if hashmap has that word or not>
      -- Here we will store each words in the map.
   Advantages in using Tries:
      1) Here we dont store the whole word, we store the characters. Thus words having same character, saves a lot of space.
      2) If the requirement of Searching a word with its alias i.e for ca -> car, camel, cage [all starts with ca]  [**IMP**]
         -- For hashmap we need to iterate the whole map to search this words starts with ca..
         -- In Tries, we store char by char, So seaching words with alias name is faster.  

Tries Important Questions:
   1) Longest Common Prefix [use Normal Loop OR Trie [space optimized]]
   2) Implement a phoneBook Directory [**HARD, IMP**][Trie Insertion + Trie Traversal]


****** Phase - 3  [ Graphs, Dynamic Programming and BackTracking ]******


** BackTracking **
Concept: 
   -- Use Recursion to Traverse all the Possible Solution and Out of them select the best solution.
What ?
   -- Basically Backtracking, Mostly includes, Recursion + isVisited Array or List [keep track of visited element or Nodes]
   -- keeping Visited Array Allows not to process already Processed info.

Important Questions:
   1) Rat in the Maze [Using backTracking] [NOTE: Already solved once in Recursion]
   2) N Queen Problem [BackTracking] [**HARD**] [**IMP**]
   3) Sudoko Solver [BackTracking] [**Hard**] [**VVIMP**]




*** Graphs ***


What ?
   -- Graphs are simply the collection of some Nodes and edges.
-- Nodes/vertex -> It represent a DS that store the data for the graph. Can be char, int or object etc
-- edges -> The Connection between the Nodes is called edges.

Types:
   1) Directed Graph
      -- The Connection between the Nodes are uni-directional.
      -- Represented by directed arrow i.e  u ----> v [u and v are the nodes, from u to v we can go but vise-versa not possible]
   2) unDirected Graph
      -- The Connection between the Nodes are bi-directional.
      -- Represented by a noraml Line, no arrows i.e u ----- v [here from u we can go to v and vise-versa]

Degree of a Node ?
   -- For Directed Graph:
      1) InDegree:  Number of edges coming to the the Node.
      2) outDegree: Number of edges going out of the Node.
   -- For unDirected Graph:
      -- The Total Number of edges connected to other Nodes from the Given Nodes.

Weighted edges & unWeighted edges
   -- Edges may contains some weight. i.e u --6-- v
   -- If the Edges dont contains any weight, we can assume the weight to be 1.   

Path in the Graph:
   -- The Nodes Path that we get while traveling from source to destination Node.
   NOTE: In the path, we can only have a node appearing only 1 time. [we cannot include any cycles in the path]

Cyclic and Acyclic Graph:
   -- In a graph, If a path exist that we start from a node and we can travel back to the same Node, [Its a cyclic Graph]
   -- If no such Path exist, it's an Acyclic Graph.

Implementation of Graph:
   -- WE know the number of Nodes n, total Number of edges: n and then all the edges connected 
   -- 2 ways of Implementing: [Need to keep Record of the Nodes and it's respective connected edges]
      1) Adjacent Matrix:
         -- Here so we have a 2-D matrix of nxn size [n is the total number of Nodes] [TC: O(n^2)]
         -- Initially all the boxes are 0, representing there is no connections between the nodes.
         -- If a connection exisit between any two Nodes eg: u --5-- v
            -- Here, we need to update the matrix accordingly:
               board[u][v] = 5 
               board[v][u] = 5 [Since it's a non-directed Graph so edge is bi-directional]

      2) Adjacent list: TC: O(n) [number of Nodes]
         -- Here we use a HashMap with the syntax as: map<Node1, list of all edges from Node1>
         -- We Iterate all the edges and accordingly,
            -- We insert the Node as the key of the map and we push the edges from the Node in the list corresponding the Node.
         -- For unordered Map, Operations like find, insert, delete are faster with TC: O(1)

[**IMP**]
What is a Graph Components or Disconnected Graphs ?
   -- It is not mandatory that we have edges connecting all the Nodes/vertexs for a given graph.
   -- WE have have Nodes from 1 to N, we have edges connecting 1 to N/2 Nodes and then we have edges connecting N/2 to N Nodes. [example]
      -- In this case, we get 2 different Components of the Graphs.
   -- Overall, The Graph is disconnected, as not all the Nodes are part of only 1 components i.e we have multiple components.
Solution:
   -- To Traverse or keep track of all the Nodes, Generally in we use a loop to iterate all the Nodes from 1 to N.
   -- And apply the logic for each Nodes, For both the Components of the Graph.
   -- We maintain a visited Nodes Map, That make sure, we dont process the same Node 2 times.

NOTE:
   -- A graph Can be represented, identified by the adjList. [It holds Nodes-edges Data]

** Graph Traversals:
   1) BFS: Breadth First Search Traversal. [TC: O(n+m) SC: O(n+m), n is no of Nodes and m is number of edges]
      -- In this, we start from the source Node, WE print the adjacent or Connected Nodes to the Source Nodes.
      -- Then we Process the Adjacent Nodes one by one, till we Process all the Nodes.
      -- 3 things we need for BFS:
         1) An Adjacent List Map for Nodes-edges Relations.
         2) A Map <Node, isVisited> to keep track of all the vistied Nodes.
         3) A Queue, to Push the adjacent Nodes inside the Queue to process them.

   NOTE: If Given a disconnected Graph, Run a for loop for all the Nodes, If not visited -> call the BFS function.

   2) DFS: DEpt First Search Traversal. [TC: O(N+E), SC: O(N+E) -> linear Complexity] [uses Recursion]
      -- This method we print the Nodes in a one-by-one manner, using REcursion.
      -- 3 things we need for DFS:
         1) An Adjacent List Map for Nodes-edges Relations.
         2) A Map <Node, isVisited> to keep track of all the vistied Nodes.
         3) A Recursive Function to Recursively Call to Go to all the Nodes if not Visited and print them.

** Cyclic Detection in Graph Traversal [**IMP**]
1) In Undirected Graph: [TC, SC -> Linear]
   Logic: 
      -- Normal BFS/DFS Graph Traversal + A Map to keep track of the Parent of the current Node.
      Case:
         1) If the Node is Visted Already and it's a parent of the current node in the map, Ignore the Node.
         2) If the Node is Visted but Node is not the parent of the current Node, Cycle exists. [**IMP**]

2) In Directed Graph:
   1) Using DFS Traversal: [TC, SC: O(n+m) -> Linear]
      -- We use an bool Map, to keep track of Nodes which are going in-out of the dfs Recusrive call Stack.
      -- WhenEver we call the Recursive function dfs, dfs_call_stack Map[node] -> true. When you finish the call, dfs_call_stack[node] -> false.
      -- This keeps track, Which Node is inside the dfs_Call and is under processing. 
      Case:
         -- If a Node is already Visited and is already true for the dfs_call Stack, Then Cycle is present.
         -- If a Ndoe is alreday Visited and not in the dfs_call Stack, Ignore it.

   2) Using BFS Traversal: [Using Kahn's Algorithm] [TC, SC: O(n+m) -> Linear]
      -- If Kahn's Algorithm is giving a valid Topology Nodes -> The Graph has to be Acyclic i.e No Cycles Present.
      -- If After the Kahn's Algorithm,
         -- Number of Nodes printed from Kahn's algorithm != Number of Total vertex [The Graph Contains Cycle]


** Topology Sort in Grap ** [Only for DAG i.e Directed Acyclic Graphs] [**VVIMP**]
What ?
   -- In Topology Sort, we put the nodes of the Graph such that, For all the u---> v edges given, u should always comes before v.

Why Only for DAG ?
   -- In case of a cyclic graph, It is not possible to satisfy the above condition.
   -- Topology Sort can be used to detect if a Graph has cycles or not in a directed Graph.
      -- If we are able to find a valid Topology Sort, -> No cycle Present. Else Cycle is present.

Implementation:
   1) Using DFS Traveral. [using Stack] [TC, SC: O(n+m) -> Linear]
      -- Do the Normal DFS Traversal
      -- In the Recursive dfs Fucntion, while coming back, Store the Node in the stack.
      -- Stack Stores the Nodes in LIFO Order -> [** Topology Sorted Nodes -> DONE **]

NOTE: When we push in the stack a node, It means, We cannot move to any other node from my curr Position.
      Thus, the DFS stack topology, Works on this principle.

   2) Using BFS Traversal. [Kahn's Algorithm] [TC, SC: O(n+m) -> Linear]
      -- WE need addition DS, A int vector that Stores the in-Degree for all the Nodes of the graph.
         NOTE: IN-Degree: Number of edges coming at the current Node.
      -- Push all the Nodes with In-Degree in the Queue.
      -- Do this till Queue is empty:
         -- Pop the Queue, As we Pop the front Node, Update the In-Degree for the linked Nodes.
         -- As you Pop, Store the ans.
      [** Topology Sort using BFS --> DONE **]

** Shortest path in Graph ** [**VVVIMP**]

   1) In Undirected Graph: [BFS + Track Parent For each Node] TC, SC: Linear
      Approach:
         -- BFS Traversal Has an advantage of finding the Shortest path between any 2 Nodes. [** VVIMP **]
         -- Do BFS Traversal, Track the Parent for each Node. [use unorder_map]
         -- In the Parent, BAckTrack From Destination Node to Source Node.
         -- Reverse the ans [source to destination Shortest path]

NOTE: If the Graph has weighted edges, the structure of adjList will change 
      i.e unorder_map<u, list<pair<v, edgeWeight>>> [u and v are connected nodes]

   2) Directed Acyclic Graph [Weighted Graph]: [Topology stack + distance Tracker] [** IMP **] TC,SC -> Linear
      Approach:
         -- Find the Topology Stack [use DFS Traversal]
         -- Maintain a MinDistance array with all elements starting with INT_MAX [Infinity].
         -- Mark the MinDistance of Source Node -> 0.
         -- Process the stack:
            -- Take out a node from stack.
            -- If MinDistance[node] Not an Infinity, then add the edgeWidth + MinDistance[node] -> totalDistance.
            -- If totalDistance > MinDistance[node] -> ignore it.
               else Update the MinDistance[node] -> totalDistance.
      -- After all Processing, You will have the min Distance to cover from source to all Other Respective Nodes. [** DONE **]

** GRaph Algorithms ** [** VVVVVIMP **]

1) Dijkstra's Algorithm [Shortest path Algorithm] [TC: O(e log v) SC: O(e+v), v->no of vertex, e -> no of edges]
   -- You need:
      1) The adjList for the given Graph.
      2) A min_Distance Map for all the Nodes initialized with all Inf.
      3) A DS that stores the pair<distance, node> [Stores in such a way, we can always pull out the ele with min edge_weight]
         -- 2 Possible DS we can use:
            1) set<>: Stores the ele in sorted way i.e the first element will be always smaller.
            2) priority_queue i.e min_heap: When we pop, we always get the min element from the heap.
   Logic:
      -- Start with source node, make the min_dis of source Node 0, and push the same in the Set/minHeap.
      -- Now till the heap is empty
         -- Pull the node with least edge_weight from the set/minHeap.
         -- calculate the new dis, if it's less than the dis stored in min_dis map, update the min_dis map for the node.
[ You have the min_dis from Source Node to all other Nodes ]

NOTE:
   -- While solving, we might run into Time Exceed Error.
   -- If using vector in code, Always specify the size of the vector. eg: vector<int> min_dis(vertices);
   -- If you ignore it, might throw runTime errors.


** What is a Spanning Tree ? [** IMP **]
   -- If a Given Graph can be converted into a Tree, with n nodes and n-1 edges, where all n nodes are connected to each other.
   -- This is called a spanning tree.

** What is a Minimum Spanning Tree (MST) ?
   -- In a spanning Tree, MST refers to the spanning Tree where the sum of all the edges in minimum.

** Implementation Algorithms: Primes OR Khruskal's 

2) Prim's Algorithm: [Finding the MST for a Graph] 
   -- Needs 3 DS:
      1) A key DS: Stores the min_edge Distance For the given Node, Initialise all with Inf [map or vector]
      2) A MST DS: Keep Track of Nodes which we process for MST. Initialise all with False [map or vector]
      3) A parent DS: Keep Track of Parent Nodes for the Nodes we process for MST. Initialise all with -1 [map or vector]
NOTE:
   -- Always check, if the given Nodes start from 0 to N [using vector DS will be faster] or 1 to N [unordered_map works here or Vector with extended index i.e 0 to N+1]

   Approch:
      -- Start with the source Node, mark key[source] = 0, parent[source] = -1.
      -- Iterate the key, find the node with the least distance -> minNode
      -- Update the MST[minNode] = true.
      -- Then Iterate the adjList Node for the minNode.
         -- If the edgeDistance < key[node] and Mst[node] is false, update that node's edge in the key DS and update the parent DS.
      -- Repeat from step 2.

   -- At the end, You have parent DS, containing the node-parent mapping and key DS containing the respective edge details.
   -- using this, we can construct a MST. [** DONE **]

   TC: O(n^2) [For finding the minNode, we are iterating again],  SC: O(n+M)

** Optimization:
   -- Have another DS: Priority Queue i.e minHeap, that stores the key DS elements. -> when we pop, we get the node with the least edge.
   -- It will always keep the node with minEdge at the top. -> O(log n) [Fetching minNode is faster]

   TC: O(n log n) [using priority Queue to find minNode], SC: O(n+m) [Optimised]

** Dis-Joint Set ** [Very Important]

What and Why ?
   -- Dis-Joint Set is a type of Data Structure. Mostly used for Grpahs with Multiple Components [i.e All Nodes are not connected to each other]
   -- It is the working principle behind implementation of Khruskal's Algorithm.
Use:
   1) Used in Khruskal's Algorithm for finding the MST of a Graph.
   2) It can be used to detect presence of any cycles in the Graph.
   3) If 2 random Nodes are given eg: u and v [** IMP **]
      -- It can be used to identify, if u and v belong to the same Component or different Component.

Operations:
   1) findParent() or findSet()
      -- Maintains a parent DS, that keep Tracks of the parent of the Nodes.
      -- Initially, All Node's Parent are themselves, we update it whenever we merge 2 components.

   2) union() or unionSet()
      -- This takes 2 nodes say u and v, Find their respective Parent of u and v, comapre their rank and based on it, merge them.
      -- We maintain a Rank DS.
         -- Initially Rank of all Nodes are set to 0.
   Cases:
      1) If Rank of 2 parents are same:
         -- Assign parent[u] -> parent [v] and increment the rank of parent[v]++ 

      2) If Rank of Parents are not same: [The parent with smaller Rank goes under the Parent with bigger Rank]
         1) if Rank[parent[u]] < Rand[parent[v]] : Assign parent[u] -> parent[v]
         2) if Rank[parent[v]] < Rand[parent[u]] : Assign parent[v] -> parent[u]

[** VIMP **]
NOTE: 
   -- The above Implementation of Union is called "Union by Rank and path compression". [** VIMP **]

   1) We have a scenario, After union of say 2 nodes, we have a path that follows: 6 -> 7 -> 4
      -- Here, parent[7] -> 4, parent[6] -> 7, parent[7] -> 4 i.e parent of 6 is also 4.
      -- This extra traversal to find the parent can be fixed, as 7 -> 4 <- 6.
      -- Now, both parent[7] = parent[6] = 4. [** Path Compression Algorithm **]
   
   2) How to find if 2 random Nodes belong to the same Component or not ?
      -- Find the parent of both the nodes.
      -- If they have the same parent i.e they belong to the same component of the graph.
      -- If they have different compoenent i.e they belong to different components.


3) Khruskal's Algorithm: [Finding MST for a Graph]
Steps:   
   -- NOTE: This algorthim we dont need any adjList
   -- WE need a linear DS, that stores the [edge_weight, u, v] in ascending Order i.e [The elements should be sorted based on their edge weight]
   -- Now Run a loop, till we process all the edges,
      1. Find parent of u and v.
      Case:
         1) If same Parent: They belong to the same MST. [ignore]
         2) If different Parent: Merge both of them using unionSet() from disjoint set Data structure
   -- Keep track of the weights while merging 2 nodes.
   -- At the end you will have the Final MST edge wight for the Given Graph [** DONE **]

** Bridge in a Graph **
what ?
   -- Condider a Graph. A Bridge refers to a edge, Which on remove, split the whole graph into Components.[i.e Makes the Graph disconnected or increase the number of components in the graph]

Implementation: TC: O(N+V) SC: O(N) [Liner]
   -- 3 Data Structure needed:
      1) DiscoveryTime: can be vector or map, keep track of the maxTime taken to reach a particualr Node.
      2) Low: can be vector or map, keep track of the miniTime taken to reach a particualr Node.
      3) Visited: A map keep tracks of the visited Nodes.
Logic:   
   -- You Do a noraml DFS in the given Graph, maintain a timer, update the timer everytime You visit a new Node.
   -- Also maintain the track of the parent Node.
   -- Update the same in the DiscovertTime DS, and LowDiscoveryTime DS, increment the timer.
   -- Iterate all the adjList Nodes for the currNode.
      -- If the node is parent of a currNode -> ignore it.
      -- If the node is Visited but not a parent. i.e Back Edge Condition.[IT means there exisit another path to reach the node, which dont include the currNode]
         -- In this case update the low[currNode] = min(low(node), DiscovertTime(neighbour))
      -- While returning back, 2 Things:
         1) Update the low for the neighbour node: low[neighbour] = min(low(node), low(neighbour)) [the neighbour might form a backedge, thus need to update the low for this node too]
         2) check for bridge condition: low[neighbour] > DiscovertTime[node] 
            -- If true Bridge Exist.
         Reason: This works because, This condition means there exist only one path to the node. If there exisit another path, low would be less.
   [** DONE **]


** Articulation point **
what ?
   -- Consider a Graph, An Articulation point/Node refers to a node, which when removed split the whole graph into Components.

Implementation: TC: O(N+E) SC: O(N) [Linear]
   -- Implementation is similar to Finding Bridge in the Graph.
   -- DS Needed:
      1) discTime: vector/map, keep tracks of the maxTime needed to Reach a particular Node.
      2) low: vector/map, keep tracks of the miniTime needed to Reach a particular Node.
      3) isVisited: A Map to keep Track, if a node is visited or not.   
Logic:
   -- You Do a noraml DFS in the given Graph, maintain a timer, update the timer everytime You visit a new Node.
   -- Also maintain the track of the parent Node.
   -- Update the same in the DiscovertTime DS, and LowDiscoveryTime DS, incement the timer.
   -- Keep track of child for this node.
   -- Iterate all the adjList Nodes for the currNode.
      -- If the node is parent of a currNode -> ignore it.
      -- If the node is Visited but not a parent. i.e Back Edge Condition.[IT means there exisit another path to reach the node, which dont include the currNode]
         -- In this case update the low[currNode] = min(low(node), DiscovertTime(neighbour))
      -- While returning back, 2 Things:
         1) Update the low for the neighbour node: low[neighbour] = min(low(node), low(neighbour)) [the neighbour might form a backedge, thus need to update the low for this node too]
         2) check for Articulation condition: low[neighbour] >= DiscovertTime[node] && parent != -1 
            -- If true Node is a Articualate Point, add the node in the Articulate Point List.
         Reason:  This works because, This condition means there exist only one path to the node. If there exisit another path, low would be less.
   -- Check if the node is sorce Node i.e parent is -1 and it's child count > 1
      -- Include source Node in the Articulation Point array.
   [** DONE **]

** Strongly Connected Components ** [Kosaraju's Algorithm]
what ?
   -- A strongly Connected component is where you take any Node, You can cover all the Other Nodes.
   -- This case can be implemented in case of a Directed Graph.

Implementation: Kosaraju's Algorithm. [TC, SC: O(N+E) where N-> total Vertices, M-> total Edges]
Logic: 3 Steps:
   1) First you need to Find the DFS Topology Stack for the Given Graph.
   2) Reverse all the edges in the Graph. i.e u -> v to v->u for all the Nodes. [also Called as Transpose the Graph]
   3) Now Based on the Stack Nodes, You Do noraml DFS traversal
      -- EAch DFS Traversal will Print all the strongly connected Nodes belong to a particular Component.

** Shortest Distance in Graph with negative Weights ** [Bellman Ford Algorithm]
What ?
   -- Bellman Ford Algo is used to Find the shortest Path between src and Destination in a Directed Graph with Negative Weights.
NOTE: If Grpah is unDirected, Convert it to Directed. i.e u ---wt--- V  => u --wt--> v & v --wt--> u

Why Not Dijkstra Algorith ?
   -- Dijkstra Algo is also used to Find the Shortest Path, But in Case of edges having negative Weights, It will Give worng Answers.

Use ?
   1) In a Directed Graph with negative edges: find shortest path between src and dest Node. 
   2) Detect Negative Cycles in the Graph:
      -- If a Graph is given, There exist a cycle in it, with negative Edges.
      -- Everytime We loop the nodes in that cycle, The distance keeps on Reducing. [Thus, In this case, No Valid short Path exist among the Nodes]

Implementation:
   NOTE: 
   -- You dont need adjList for Bellman Ford Algorithm. You need the edges and it's respective weights.
   -- You need to maintain a Vector to keep track of shortest Distance i.e dis vector. Initialize with Inf i.e INT_MAX, excpet for src Node -> 0
Steps:   
   1) If there are N Nodes, You need to Check the below n-1 Times...
      -- For All the edges eg: u ---wt---> v
         if(dist[u] + wt < dist[v]) -> Update the dis[v] = dist[u] + wt;
      [** DONE-> you found the MinDist form Src to all Other Nodes]
   2) To check if There exist a Negative cycle in the Grpah.
      -- Run the above check one more time, If any dis is updating, There exist a negative Cycle, So valid Shortest path won't exist.

** Dynamic Programming - DP ** [Most Important topic]

When to use DP ?
   -- A Problem falls under DP under 2 condition:
      1) Overlapping sub-problems: same functionality is repeated again and again.
      2) The Problem can be solved with the optimal Solutuion of the sub-problems.

NOTE: 
   -- BAsically if a problem can be solved using Recusion in TC:O(2^N), there is a chance we are doing repeated processing of the same thing again and again.
   -- This grows the tree dept and also increases the TC.
   -- DP tech of memorization, Reduce the dept of the Tree by storing the result of Recursive Fn that is already calculated once.

Implementation of DP
   -- DP implementation includes 3 Things:
      1) Top Down Approach: Includes Recursion + Memorization
         -- Normal Recursion we will solve. To avoid the Recursion Tree for the same input, WE will store the result i.e memorization.
      2) Bottoms Up Approach: Includes Tabulation, can be 1D or 2D depending upon the problem 
         -- We avoid Recusion, We use 1-D/2-D data structure to store the data.
      3) Space Optimization: Optimized the Space Complexity
         -- We try to minimize the space used.

** Example: check Fibbo Code for it's implementation in all the above 3 approaches.

Logic:
-- While Solving Dp, Follow the steps Sequentially:
   1) First Find a Recursise Solution to the problem. [TC: exponential i.e 2^n]
   NOTE: Find the Recursive Relation between the problem and sub-problem.
   
   2) In the Recursion, to reduce the dept of the Tree, Use Memorization, to store Recursive Result.
      -- use Vector or MAP to store the result and retrive it whenever needed. TC: Linear i.e O(N) but space: O(N) + O(N) [One for vector + One for the REcursive Stack]
      -- This called Memorization or Top Bottom Appraoch [i.e Moving from top to bottom of the Recursive Tree]
   NOTE: Steps: Take Reference from Recursive Solution, Store the res in dp array, after base case if the dp holds the value, If yes return it.
   
   
   3) Now To Remove the, Recursion Form the solution, Convert it into a looping logic.
      -- Here we use still use Vector or MAP but, we will not depend upon Recursion any More.
      -- This is called Tabulation or Bottom Up Appraoch i.e Moving from bottom to top of the Recursive Tree.
      -- TC: Linear i.e O(N) and SC: O(N) [Space taken by vector]
   NOTE: Steps: Create a dp array, analysis the base case, Reference from Recursion + Memorization, convert in looping logic.
   
   4) Space Optimization [Optional, If possible]
      -- If Possible, WE will still use the loop, but we will Try to remove the vector from the logic.
      -- Use variables + loop Logic to handle the Values.
      -- TC: O(N) [Loop is used], SC: O(1)
   NOTE: Check the realtion of dp[n] on it's dependent. If it's consistent like dp[n] always depend on dp[n-1] and dp[n-1] -> Can be space Optimized.

NOTE:
   1-D Dp Problems:
      -- Here the Problem's Recursion Tree can be optimised with Memorization and Tabulation using a Linear DP Vector.
      -- Here the result is dependent on the change of only 1 constaraint i.e N
      -- In the Recursive Call, Only 1 state keeps on updating in every fun call stack thus, It's 1-D DP

Basic Questions: [1-D DP Problems]
   1) nth Fibbo Number. [f(N) = F(N-1) + F(N-2) -> Recursive Relation]
   2) MinCost to climb stairs. [**IMP**] [F(N) = Cost[N] + min(F(N-1), F(N-2)) -> Step Cost and Min of the prev 2 steps]
   3) Minimum Element i.e coin Exchange problem [**VVIMP**] 
   4) combination Sum IV [Based on Choice, You either pick an element or not, Recursive Tree] [**IMP**]

Important:
   5) MaxSum for Non-Adjacent Elements. [Based on Choice, You either pick an element or not, Recursive Tree]
   6) House-Robber 2 Problem.[Implementation of MaxSum for Non-Adjacent Element's Logic] [**VIMP**]
   7) Count Segments of a Rod. [based on choice of the segment, Recursive choice Tree] [**IMP**]
   8) Possible de-arrangments [based on Recursion Relation derivation] [**VIMP**]
   9) Ninja & Fence problem [Painting Fecnce Algorithm, 1-DP] [**VIMP**]
   10) Combination Sum [Each tar will have choices, Recurive choice Tree][**VIMP**]
   10.1) Combination Sum, Return all the possible Pairs [Same Logic as above + Backtracking] [**VVIMP**]
   11) Perfect Squares [Recursive choice tree] [**VIMP**]
   12) Minimum cost of Ticket [Recursive choice Tree] [**VVVVIMP**]
   13) Catalan Number Implementation: [Cn = C0Cn-1 + C1Cn-2 + ..... Cn-1C0] [** VVIMP **]
         -- Implementaion [using Recursion, Memorization[Top-down] and Catalan Triangulation using Dp]
         -- Possible Questions:
            - No of Possible BSTs based on number of Nodes. => Cn
            - No of ways of Arranging valid N-parenthesis based on given set of Parenthesis. -> Cn
            - Mountain Ranges [Possible ways to Arrange Mountains to stay above ground/sea-level] -> Cn
            - Number of convex Polygon. -> Cn-2 [**VIMP**]

NOTE:
   -- 2-D Problems: Here the Recursion for the problem has 2 states which will be changing on each Function call stack.
   -- We need to maintain 

Imporant:
   1) 0 1 Kanpsack Problem [logic: either include or exclude the item in Recursion Tree / 2D DP] [** VVIMP **]
      -- Problem on same pattern:
         1)Equal Subset sum partition
         2)Subset Sum
         3)Min subset sum difference
         4)count of subset sum
         5)targetSum 
      NOTE: aLl this problems can be solved based on Logic of either include ot exclude the element + Recursive Tree / 2D DP
   
   2) Maximal-square [Logic: min of right, dig and bottom of the curr Position] [** VVVIMP **]

   3) Matrix Chain Multiplication Sum Appraoch: [MCMS Problem] [*** VIMP ***]
      -- Find the Minimum Number of Operations needed to Perform Matrix Multiplications. [MCMS Implementation using bottom-up Approcah] 
      -- Min Score Triangulation of Polygon. [Solved using dp and MCMS Appraoch] [** VVVIMP **]

   4) 